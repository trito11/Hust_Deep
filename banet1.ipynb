{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7153471,"sourceType":"datasetVersion","datasetId":4130659},{"sourceId":7165240,"sourceType":"datasetVersion","datasetId":4139055},{"sourceId":7173542,"sourceType":"datasetVersion","datasetId":4145006},{"sourceId":7177819,"sourceType":"datasetVersion","datasetId":4148253}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":7774.718192,"end_time":"2023-11-13T09:21:28.081859","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-11-13T07:11:53.363667","version":"2.4.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchgeometry\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":5.358224,"end_time":"2023-11-13T07:12:26.531689","exception":false,"start_time":"2023-11-13T07:12:21.173465","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip show torch\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# from torchgeometry.losses import one_hot\nimport os\nimport os.path as osp\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport cv2\nimport time\nimport imageio\nimport matplotlib.pyplot as plt\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch import Tensor\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision.transforms import Resize, PILToTensor, ToPILImage, Compose, InterpolationMode\nfrom collections import OrderedDict\nimport wandb\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport glob\nimport multiprocessing.pool as mpp\nimport multiprocessing as mp\nimport argparse\nimport torch\nimport albumentations as albu\nfrom torchvision.transforms import (Pad, ColorJitter, Resize, FiveCrop, RandomResizedCrop,\n                                    RandomHorizontalFlip, RandomRotation, RandomVerticalFlip)\nimport random\nimport math\nimport numbers\nfrom PIL import Image, ImageOps, ImageEnhance\nimport numpy as np\nimport random\nfrom scipy.ndimage.morphology import generate_binary_structure, binary_erosion\nfrom scipy.ndimage import maximum_filter\nfrom tqdm import tqdm\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport os\nimport torch\nfrom torch import nn\nimport cv2\nimport numpy as np\nimport argparse\nfrom pathlib import Path\n\nfrom pytorch_lightning.loggers import CSVLogger\nimport random","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"papermill":{"duration":0.082489,"end_time":"2023-11-13T07:12:27.60011","exception":false,"start_time":"2023-11-13T07:12:27.517621","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parameters","metadata":{"papermill":{"duration":0.013953,"end_time":"2023-11-13T07:12:27.627781","exception":false,"start_time":"2023-11-13T07:12:27.613828","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nnum_classes = 6\n\n# Number of epoch\nepochs = 100\n\n# Hyperparameters for training \nlearning_rate = 8e-04\nbatch_size = 8\ndisplay_step = 2\n\n# Model path\ncheckpoint_path = '/kaggle/working/unet_model.pth'\npretrained_path = \"/kaggle/input/model2/unet_model (1).pth\"\n# Initialize lists to keep track of loss and accuracy\nloss_epoch_array = []\ntrain_accuracy = []\ntest_accuracy = []\nvalid_accuracy = []","metadata":{"papermill":{"duration":0.021712,"end_time":"2023-11-13T07:12:27.662876","exception":false,"start_time":"2023-11-13T07:12:27.641164","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 42\n\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\nseed_everything(42)\n\nImSurf = np.array([255, 255, 255])  # label 0\nBuilding = np.array([255, 0, 0]) # label 1\nLowVeg = np.array([255, 255, 0]) # label 2\nTree = np.array([0, 255, 0]) # label 3\nCar = np.array([0, 255, 255]) # label 4\nClutter = np.array([0, 0, 255]) # label 5\nBoundary = np.array([0, 0, 0]) # label 6\nnum_classes = 6\n\n\n\n\n\ndef pv2rgb(mask):\n    h, w = mask.shape[0], mask.shape[1]\n    mask_rgb = np.zeros(shape=(h, w, 3), dtype=np.uint8)\n    mask_convert = mask[np.newaxis, :, :]\n    mask_rgb[np.all(mask_convert == 3, axis=0)] = [0, 255, 0]\n    mask_rgb[np.all(mask_convert == 0, axis=0)] = [255, 255, 255]\n    mask_rgb[np.all(mask_convert == 1, axis=0)] = [255, 0, 0]\n    mask_rgb[np.all(mask_convert == 2, axis=0)] = [255, 255, 0]\n    mask_rgb[np.all(mask_convert == 4, axis=0)] = [0, 204, 255]\n    mask_rgb[np.all(mask_convert == 5, axis=0)] = [0, 0, 255]\n    return mask_rgb\ndef label2rgb(mask):\n    h, w = mask.shape[0], mask.shape[1]\n    mask_rgb = np.zeros(shape=(h, w, 3), dtype=np.uint8)\n    mask_convert = mask[np.newaxis, :, :]\n    mask_rgb[np.all(mask_convert == 3, axis=0)] = [0, 255, 0]\n    mask_rgb[np.all(mask_convert == 0, axis=0)] = [255, 255, 255]\n    mask_rgb[np.all(mask_convert == 1, axis=0)] = [255, 0, 0]\n    mask_rgb[np.all(mask_convert == 2, axis=0)] = [255, 255, 0]\n    mask_rgb[np.all(mask_convert == 4, axis=0)] = [0, 204, 255]\n    mask_rgb[np.all(mask_convert == 5, axis=0)] = [0, 0, 255]\n    return mask_rgb\n\n\n\ndef car_color_replace(mask):\n    mask = cv2.cvtColor(np.array(mask.copy()), cv2.COLOR_RGB2BGR)\n    mask[np.all(mask == [0, 255, 255], axis=-1)] = [0, 204, 255]\n\n    return mask\n\n\ndef rgb_to_2D_label(_label):\n    _label = _label.transpose(2, 0, 1)\n    label_seg = np.zeros(_label.shape[1:], dtype=np.uint8)\n    label_seg[np.all(_label.transpose([1, 2, 0]) == ImSurf, axis=-1)] = 0\n    label_seg[np.all(_label.transpose([1, 2, 0]) == Building, axis=-1)] = 1\n    label_seg[np.all(_label.transpose([1, 2, 0]) == LowVeg, axis=-1)] = 2\n    label_seg[np.all(_label.transpose([1, 2, 0]) == Tree, axis=-1)] = 3\n    label_seg[np.all(_label.transpose([1, 2, 0]) == Car, axis=-1)] = 4\n    label_seg[np.all(_label.transpose([1, 2, 0]) == Clutter, axis=-1)] = 5\n    label_seg[np.all(_label.transpose([1, 2, 0]) == Boundary, axis=-1)] = 6\n    return label_seg\n\n\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # Split Data","metadata":{}},{"cell_type":"markdown","source":" # Dataloader","metadata":{"papermill":{"duration":0.013261,"end_time":"2023-11-13T07:12:27.690633","exception":false,"start_time":"2023-11-13T07:12:27.677372","status":"completed"},"tags":[]}},{"cell_type":"code","source":"CLASSES = ('ImSurf', 'Building', 'LowVeg', 'Tree', 'Car', 'Clutter')\nclass_label={0:'ImSurf', 1:'Building',2: 'LowVeg', 3:'Tree', 4:'Car',5: 'Clutter'}\nPALETTE = [[255, 255, 255], [0, 0, 255], [0, 255, 255], [0, 255, 0], [255, 204, 0], [255, 0, 0]]\n\nORIGIN_IMG_SIZE = (256, 256)\nINPUT_IMG_SIZE = (256, 256)\nTEST_IMG_SIZE = (256, 256)\n\ndef get_training_transform():\n    train_transform = [\n        # albu.RandomBrightnessContrast(brightness_limit=0.25, contrast_limit=0.25, p=0.15),\n        # albu.RandomRotate90(p=0.25),\n        albu.Normalize()\n    ]\n    return albu.Compose(train_transform)\n\ndef get_val_transform():\n    val_transform = [\n        albu.Normalize()\n    ]\n    return albu.Compose(val_transform)\n\n\ndef val_aug(img, mask):\n    img, mask = np.array(img), np.array(mask)\n    aug = get_val_transform()(image=img.copy(), mask=mask.copy())\n    img, mask = aug['image'], aug['mask']\n    return img, mask\n\n\nclass PotsdamDataset(Dataset):\n    def __init__(self, data_root='/kaggle/input/deep-data-potsdam-vaihingen/Deep_data_segmentation/Split/Potsdam/', mode='val', img_dir='Image', mask_dir='Label/',\n                 img_suffix='.tif', mask_suffix='.png', transform=val_aug, mosaic_ratio=0.0,\n                 img_size=ORIGIN_IMG_SIZE):\n        self.data_root = data_root\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.img_suffix = img_suffix\n        self.mask_suffix = mask_suffix\n        self.transform = transform\n        self.mode = mode\n        self.mosaic_ratio = mosaic_ratio\n        self.img_size = img_size\n        self.img_ids = self.get_img_ids(self.data_root, self.img_dir, self.mask_dir)\n\n    def __getitem__(self, index):\n        p_ratio = random.random()\n        if p_ratio > self.mosaic_ratio or self.mode == 'val' or self.mode == 'test':\n            img, mask = self.load_img_and_mask(index)\n            if self.transform:\n                img, mask = self.transform(img, mask)\n        else:\n            img, mask = self.load_mosaic_img_and_mask(index)\n            if self.transform:\n                img, mask = self.transform(img, mask)\n\n        img = torch.from_numpy(img).permute(2, 0, 1).float()\n        mask = torch.from_numpy(mask).long()\n        img_id = self.img_ids[index]\n        results = dict(img_id=img_id, img=img, gt_semantic_seg=mask)\n        return results\n\n    def __len__(self):\n        return len(self.img_ids)\n\n    def get_img_ids(self, data_root, img_dir, mask_dir):\n        img_filename_list = os.listdir(osp.join(data_root, img_dir))\n        mask_filename_list = os.listdir(osp.join(data_root, mask_dir))\n        assert len(img_filename_list) == len(mask_filename_list)\n        img_ids = [str(id.split('.')[0]) for id in mask_filename_list]\n        return img_ids\n\n    def load_img_and_mask(self, index):\n        img_id = self.img_ids[index]\n        img_name = osp.join(self.data_root, self.img_dir, img_id + self.img_suffix)\n        mask_name = osp.join(self.data_root, self.mask_dir, img_id + self.mask_suffix)\n        img = Image.open(img_name).convert('RGB')\n        mask = Image.open(mask_name).convert('L')\n        return img, mask\n\n    def load_mosaic_img_and_mask(self, index):\n        indexes = [index] + [random.randint(0, len(self.img_ids) - 1) for _ in range(3)]\n        img_a, mask_a = self.load_img_and_mask(indexes[0])\n        img_b, mask_b = self.load_img_and_mask(indexes[1])\n        img_c, mask_c = self.load_img_and_mask(indexes[2])\n        img_d, mask_d = self.load_img_and_mask(indexes[3])\n\n        img_a, mask_a = np.array(img_a), np.array(mask_a)\n        img_b, mask_b = np.array(img_b), np.array(mask_b)\n        img_c, mask_c = np.array(img_c), np.array(mask_c)\n        img_d, mask_d = np.array(img_d), np.array(mask_d)\n\n        w = self.img_size[1]\n        h = self.img_size[0]\n\n        start_x = w // 4\n        strat_y = h // 4\n        # The coordinates of the splice center\n        offset_x = random.randint(start_x, (w - start_x))\n        offset_y = random.randint(strat_y, (h - strat_y))\n\n        crop_size_a = (offset_x, offset_y)\n        crop_size_b = (w - offset_x, offset_y)\n        crop_size_c = (offset_x, h - offset_y)\n        crop_size_d = (w - offset_x, h - offset_y)\n\n        random_crop_a = albu.RandomCrop(width=crop_size_a[0], height=crop_size_a[1])\n        random_crop_b = albu.RandomCrop(width=crop_size_b[0], height=crop_size_b[1])\n        random_crop_c = albu.RandomCrop(width=crop_size_c[0], height=crop_size_c[1])\n        random_crop_d = albu.RandomCrop(width=crop_size_d[0], height=crop_size_d[1])\n\n        croped_a = random_crop_a(image=img_a.copy(), mask=mask_a.copy())\n        croped_b = random_crop_b(image=img_b.copy(), mask=mask_b.copy())\n        croped_c = random_crop_c(image=img_c.copy(), mask=mask_c.copy())\n        croped_d = random_crop_d(image=img_d.copy(), mask=mask_d.copy())\n\n        img_crop_a, mask_crop_a = croped_a['image'], croped_a['mask']\n        img_crop_b, mask_crop_b = croped_b['image'], croped_b['mask']\n        img_crop_c, mask_crop_c = croped_c['image'], croped_c['mask']\n        img_crop_d, mask_crop_d = croped_d['image'], croped_d['mask']\n\n        top = np.concatenate((img_crop_a, img_crop_b), axis=1)\n        bottom = np.concatenate((img_crop_c, img_crop_d), axis=1)\n        img = np.concatenate((top, bottom), axis=0)\n\n        top_mask = np.concatenate((mask_crop_a, mask_crop_b), axis=1)\n        bottom_mask = np.concatenate((mask_crop_c, mask_crop_d), axis=1)\n        mask = np.concatenate((top_mask, bottom_mask), axis=0)\n        mask = np.ascontiguousarray(mask)\n        img = np.ascontiguousarray(img)\n\n        img = Image.fromarray(img)\n        mask = Image.fromarray(mask)\n\n        return img, mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_dataset = PotsdamDataset(data_root='/kaggle/input/deep-data-potsdam-vaihingen/Deep_data_segmentation/Split/Potsdam/', mode='train',\n                               mosaic_ratio=0.25, transform=val_aug)\n\ntrain_size = 0.35\nvalid_size = 0.65\ndumb1 = 0.15\ndumb2 = 0.85\ntrain_length = round(train_size * len(train_dataset))\nvalid_length = round(valid_size * len(train_dataset))\ntrain_set, dumb0 = random_split(train_dataset, [train_length, valid_length])\nval_set, xxx = random_split(dumb0, [dumb1, dumb2])\n\n\ntest_dataset = PotsdamDataset(data_root='/kaggle/input/deep-data-potsdam-vaihingen/Deep_data_segmentation/Split/Potsdam/Test',\n                              transform=val_aug)\n\ntrain_loader = DataLoader(dataset=train_set,\n                          batch_size=batch_size,\n                          num_workers=0,\n                          pin_memory=True,\n                          shuffle=True,\n                          drop_last=True)\n\nval_loader = DataLoader(dataset=val_set,\n                        batch_size=batch_size,\n                        num_workers=0,\n                        shuffle=False,\n                        pin_memory=True,\n                        drop_last=False)\n","metadata":{"papermill":{"duration":0.019707,"end_time":"2023-11-13T07:12:27.79702","exception":false,"start_time":"2023-11-13T07:12:27.777313","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metric","metadata":{}},{"cell_type":"code","source":"class Evaluator(object):\n    def __init__(self, num_class):\n        self.num_class = num_class\n        self.confusion_matrix = np.zeros((self.num_class,) * 2)\n        self.eps = 1e-8\n\n    def get_tp_fp_tn_fn(self):\n        tp = np.diag(self.confusion_matrix)\n        fp = self.confusion_matrix.sum(axis=0) - np.diag(self.confusion_matrix)\n        fn = self.confusion_matrix.sum(axis=1) - np.diag(self.confusion_matrix)\n        tn = np.diag(self.confusion_matrix).sum() - np.diag(self.confusion_matrix)\n        return tp, fp, tn, fn\n\n    def Precision(self):\n        tp, fp, tn, fn = self.get_tp_fp_tn_fn()\n        precision = tp / (tp + fp)\n        return precision\n\n    def Recall(self):\n        tp, fp, tn, fn = self.get_tp_fp_tn_fn()\n        recall = tp / (tp + fn)\n        return recall\n\n    def F1(self):\n        tp, fp, tn, fn = self.get_tp_fp_tn_fn()\n        Precision = tp / (tp + fp)\n        Recall = tp / (tp + fn)\n        F1 = (2.0 * Precision * Recall) / (Precision + Recall)\n        return F1\n\n    def OA(self):\n        OA = np.diag(self.confusion_matrix).sum() / (self.confusion_matrix.sum() + self.eps)\n        return OA\n\n    def Intersection_over_Union(self):\n        tp, fp, tn, fn = self.get_tp_fp_tn_fn()\n        IoU = tp / (tp + fn + fp)\n        return IoU\n\n    def Dice(self):\n        tp, fp, tn, fn = self.get_tp_fp_tn_fn()\n        Dice = 2 * tp / ((tp + fp) + (tp + fn))\n        return Dice\n\n    def Pixel_Accuracy_Class(self):\n        #         TP                                  TP+FP\n        Acc = np.diag(self.confusion_matrix) / (self.confusion_matrix.sum(axis=0) + self.eps)\n        return Acc\n\n    def Frequency_Weighted_Intersection_over_Union(self):\n        freq = np.sum(self.confusion_matrix, axis=1) / (np.sum(self.confusion_matrix) + self.eps)\n        iou = self.Intersection_over_Union()\n        FWIoU = (freq[freq > 0] * iou[freq > 0]).sum()\n        return FWIoU\n\n    def _generate_matrix(self, gt_image, pre_image):\n        mask = (gt_image >= 0) & (gt_image < self.num_class)\n        label = self.num_class * gt_image[mask].astype('int') + pre_image[mask]\n        count = np.bincount(label, minlength=self.num_class ** 2)\n        confusion_matrix = count.reshape(self.num_class, self.num_class)\n        return confusion_matrix\n\n    def add_batch(self, gt_image, pre_image):\n        assert gt_image.shape == pre_image.shape, 'pre_image shape {}, gt_image shape {}'.format(pre_image.shape,\n                                                                                                 gt_image.shape)\n        self.confusion_matrix += self._generate_matrix(gt_image, pre_image)\n\n    def reset(self):\n        self.confusion_matrix = np.zeros((self.num_class,) * 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"papermill":{"duration":0.013405,"end_time":"2023-11-13T07:12:28.293069","exception":false,"start_time":"2023-11-13T07:12:28.279664","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import Module, Conv2d, Parameter, Softmax\nfrom torchvision import models\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self,\n                 dim,\n                 num_heads=8,\n                 qkv_bias=False,\n                 qk_scale=None,\n                 attn_drop=0.,\n                 proj_drop=0.,\n                 sr_ratio=1,\n                 apply_transform=False):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        self.sr_ratio = sr_ratio\n        if sr_ratio > 1:\n            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio+1, stride=sr_ratio, padding=sr_ratio // 2, groups=dim)\n            self.sr_norm = nn.LayerNorm(dim)\n\n        self.apply_transform = apply_transform and num_heads > 1\n        if self.apply_transform:\n            self.transform_conv = nn.Conv2d(self.num_heads, self.num_heads, kernel_size=1, stride=1)\n            self.transform_norm = nn.InstanceNorm2d(self.num_heads)\n\n    def forward(self, x, H, W):\n        B, N, C = x.shape\n        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n        if self.sr_ratio > 1:\n            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n            x_ = self.sr_norm(x_)\n            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        else:\n            kv = self.kv(x).reshape(B, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        k, v = kv[0], kv[1]\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        if self.apply_transform:\n            attn = self.transform_conv(attn)\n            attn = attn.softmax(dim=-1)\n            attn = self.transform_norm(attn)\n        else:\n            attn = attn.softmax(dim=-1)\n\n        attn = self.attn_drop(attn)\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1, apply_transform=False):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio, apply_transform=apply_transform)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x, H, W):\n        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass PA(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.pa_conv = nn.Conv2d(dim, dim, kernel_size=3, padding=1, groups=dim)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        return x * self.sigmoid(self.pa_conv(x))\n\n\nclass GL(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.gl_conv = nn.Conv2d(dim, dim, kernel_size=3, padding=1, groups=dim)\n\n    def forward(self, x):\n        return x + self.gl_conv(x)\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\"\"\"\n    def __init__(self, patch_size=16, in_ch=3, out_ch=768, with_pos=False):\n        super().__init__()\n        self.patch_size = to_2tuple(patch_size)\n        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=patch_size+1, stride=patch_size, padding=patch_size // 2)\n        self.norm = nn.BatchNorm2d(out_ch)\n\n        self.with_pos = with_pos\n        if self.with_pos:\n            self.pos = PA(out_ch)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        x = self.conv(x)\n        x = self.norm(x)\n        if self.with_pos:\n            x = self.pos(x)\n        x = x.flatten(2).transpose(1, 2)\n        H, W = H // self.patch_size[0], W // self.patch_size[1]\n        return x, (H, W)\n\n\nclass BasicStem(nn.Module):\n    def __init__(self, in_ch=3, out_ch=64, with_pos=False):\n        super(BasicStem, self).__init__()\n        hidden_ch = out_ch // 2\n        self.conv1 = nn.Conv2d(in_ch, hidden_ch, kernel_size=3, stride=2, padding=1, bias=False)\n        self.norm1 = nn.BatchNorm2d(hidden_ch)\n        self.conv2 = nn.Conv2d(hidden_ch, hidden_ch, kernel_size=3, stride=1, padding=1, bias=False)\n        self.norm2 = nn.BatchNorm2d(hidden_ch)\n        self.conv3 = nn.Conv2d(hidden_ch, out_ch, kernel_size=3, stride=2, padding=1, bias=False)\n\n        self.act = nn.ReLU(inplace=True)\n        self.with_pos = with_pos\n        if self.with_pos:\n            self.pos = PA(out_ch)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.act(x)\n\n        x = self.conv2(x)\n        x = self.norm2(x)\n        x = self.act(x)\n\n        x = self.conv3(x)\n        if self.with_pos:\n            x = self.pos(x)\n        return x\n\n\nclass ResT(nn.Module):\n    def __init__(self, in_chans=3, num_classes=1000, embed_dims=[64, 128, 256, 512],\n                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False,\n                 qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n                 depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n                 norm_layer=nn.LayerNorm, apply_transform=False):\n        super().__init__()\n        self.num_classes = num_classes\n        self.depths = depths\n        self.apply_transform = apply_transform\n\n        self.stem = BasicStem(in_ch=in_chans, out_ch=embed_dims[0], with_pos=True)\n\n        self.patch_embed_2 = PatchEmbed(patch_size=2, in_ch=embed_dims[0], out_ch=embed_dims[1], with_pos=True)\n        self.patch_embed_3 = PatchEmbed(patch_size=2, in_ch=embed_dims[1], out_ch=embed_dims[2], with_pos=True)\n        self.patch_embed_4 = PatchEmbed(patch_size=2, in_ch=embed_dims[2], out_ch=embed_dims[3], with_pos=True)\n\n        # transformer encoder\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n        cur = 0\n\n        self.stage1 = nn.ModuleList([\n            Block(embed_dims[0], num_heads[0], mlp_ratios[0], qkv_bias, qk_scale, drop_rate, attn_drop_rate,\n                  drop_path=dpr[cur+i], norm_layer=norm_layer, sr_ratio=sr_ratios[0], apply_transform=apply_transform)\n            for i in range(self.depths[0])])\n\n        cur += depths[0]\n        self.stage2 = nn.ModuleList([\n            Block(embed_dims[1], num_heads[1], mlp_ratios[1], qkv_bias, qk_scale, drop_rate, attn_drop_rate,\n                  drop_path=dpr[cur+i], norm_layer=norm_layer, sr_ratio=sr_ratios[1], apply_transform=apply_transform)\n            for i in range(self.depths[1])])\n\n        cur += depths[1]\n        self.stage3 = nn.ModuleList([\n            Block(embed_dims[2], num_heads[2], mlp_ratios[2], qkv_bias, qk_scale, drop_rate, attn_drop_rate,\n                  drop_path=dpr[cur+i], norm_layer=norm_layer, sr_ratio=sr_ratios[2], apply_transform=apply_transform)\n            for i in range(self.depths[2])])\n\n        cur += depths[2]\n        self.stage4 = nn.ModuleList([\n            Block(embed_dims[3], num_heads[3], mlp_ratios[3], qkv_bias, qk_scale, drop_rate, attn_drop_rate,\n                  drop_path=dpr[cur+i], norm_layer=norm_layer, sr_ratio=sr_ratios[3], apply_transform=apply_transform)\n            for i in range(self.depths[3])])\n\n        self.norm = norm_layer(embed_dims[3])\n\n        # init weights\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Conv2d):\n            trunc_normal_(m.weight, std=0.02)\n        elif isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n            nn.init.constant_(m.weight, 1.0)\n            nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.stem(x)\n        B, _, H, W = x.shape\n        x = x.flatten(2).permute(0, 2, 1)\n\n        # stage 1\n        for blk in self.stage1:\n            x = blk(x, H, W)\n        x = x.permute(0, 2, 1).reshape(B, -1, H, W)\n        # x1 = x\n\n        # stage 2\n        x, (H, W) = self.patch_embed_2(x)\n        for blk in self.stage2:\n            x = blk(x, H, W)\n        x = x.permute(0, 2, 1).reshape(B, -1, H, W)\n        # x2 = x\n\n        # stage 3\n        x, (H, W) = self.patch_embed_3(x)\n        for blk in self.stage3:\n            x = blk(x, H, W)\n        x = x.permute(0, 2, 1).reshape(B, -1, H, W)\n        x3 = x\n\n        # stage 4\n        x, (H, W) = self.patch_embed_4(x)\n        for blk in self.stage4:\n            x = blk(x, H, W)\n        x = self.norm(x)\n        x = x.permute(0, 2, 1).reshape(B, -1, H, W)\n        x4 = x\n\n        return x3, x4\n\n\ndef rest_lite(pretrained=True, weight_path='pretrain_weights/rest_lite.pth',  **kwargs):\n    model = ResT(embed_dims=[64, 128, 256, 512], num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=True,\n                 depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1], apply_transform=True, **kwargs)\n    if pretrained and weight_path is not None:\n        old_dict = torch.load(weight_path)\n        model_dict = model.state_dict()\n        old_dict = {k: v for k, v in old_dict.items() if (k in model_dict)}\n        model_dict.update(old_dict)\n        model.load_state_dict(model_dict)\n    return model\n\n\nclass ConvBNReLU(nn.Module):\n    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1):\n        super(ConvBNReLU, self).__init__()\n        self.conv = nn.Conv2d(in_chan,\n                              out_chan,\n                              kernel_size=ks,\n                              stride=stride,\n                              padding=padding,\n                              bias=False)\n        self.bn = nn.BatchNorm2d(out_chan)\n        self.relu = nn.ReLU(inplace=True)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n\ndef l2_norm(x):\n    return torch.einsum(\"bcn, bn->bcn\", x, 1 / torch.norm(x, p=2, dim=-2))\n\n\nclass LinearAttention(Module):\n    def __init__(self, in_places, scale=8, eps=1e-6):\n        super(LinearAttention, self).__init__()\n        self.gamma = Parameter(torch.zeros(1))\n        self.in_places = in_places\n        self.l2_norm = l2_norm\n        self.eps = eps\n\n        self.query_conv = Conv2d(in_channels=in_places, out_channels=in_places // scale, kernel_size=1)\n        self.key_conv = Conv2d(in_channels=in_places, out_channels=in_places // scale, kernel_size=1)\n        self.value_conv = Conv2d(in_channels=in_places, out_channels=in_places, kernel_size=1)\n\n    def forward(self, x):\n        # Apply the feature map to the queries and keys\n        batch_size, chnnels, width, height = x.shape\n        Q = self.query_conv(x).view(batch_size, -1, width * height)\n        K = self.key_conv(x).view(batch_size, -1, width * height)\n        V = self.value_conv(x).view(batch_size, -1, width * height)\n\n        Q = self.l2_norm(Q).permute(-3, -1, -2)\n        K = self.l2_norm(K)\n\n        tailor_sum = 1 / (width * height + torch.einsum(\"bnc, bc->bn\", Q, torch.sum(K, dim=-1) + self.eps))\n        value_sum = torch.einsum(\"bcn->bc\", V).unsqueeze(-1)\n        value_sum = value_sum.expand(-1, chnnels, width * height)\n\n        matrix = torch.einsum('bmn, bcn->bmc', K, V)\n        matrix_sum = value_sum + torch.einsum(\"bnm, bmc->bcn\", Q, matrix)\n\n        weight_value = torch.einsum(\"bcn, bn->bcn\", matrix_sum, tailor_sum)\n        weight_value = weight_value.view(batch_size, chnnels, height, width)\n\n        return x + (self.gamma * weight_value).contiguous()\n\n\nclass Output(nn.Module):\n    def __init__(self, in_chan, mid_chan, n_classes, up_factor=32, *args, **kwargs):\n        super(Output, self).__init__()\n        self.up_factor = up_factor\n        out_chan = n_classes * up_factor * up_factor\n        self.conv = ConvBNReLU(in_chan, mid_chan, ks=3, stride=1, padding=1)\n        self.conv_out = nn.Conv2d(mid_chan, out_chan, kernel_size=1, bias=True)\n        self.up = nn.PixelShuffle(up_factor)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.conv_out(x)\n        x = self.up(x)\n        return x\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.modules.batchnorm._BatchNorm):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass UpSample(nn.Module):\n\n    def __init__(self, n_chan, factor=2):\n        super(UpSample, self).__init__()\n        out_chan = n_chan * factor * factor\n        self.proj = nn.Conv2d(n_chan, out_chan, 1, 1, 0)\n        self.up = nn.PixelShuffle(factor)\n        self.init_weight()\n\n    def forward(self, x):\n        feat = self.proj(x)\n        feat = self.up(feat)\n        return feat\n\n    def init_weight(self):\n        nn.init.xavier_normal_(self.proj.weight, gain=1.)\n\n\nclass Attention_Embedding(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(Attention_Embedding, self).__init__()\n        self.attention = LinearAttention(in_channels)\n        self.conv_attn = ConvBNReLU(in_channels, out_channels)\n        self.up = UpSample(out_channels)\n\n    def forward(self, high_feat, low_feat):\n        A = self.attention(high_feat)\n        A = self.conv_attn(A)\n        A = self.up(A)\n\n        output = low_feat * A\n        output += low_feat\n\n        return output\n\n\nclass FeatureAggregationModule(nn.Module):\n    def __init__(self, in_chan, out_chan):\n        super(FeatureAggregationModule, self).__init__()\n        self.convblk = ConvBNReLU(in_chan, out_chan, ks=1, stride=1, padding=0)\n        self.conv_atten = LinearAttention(out_chan)\n        self.init_weight()\n\n    def forward(self, fsp, fcp):\n        fcat = torch.cat([fsp, fcp], dim=1)\n        feat = self.convblk(fcat)\n        atten = self.conv_atten(feat)\n        feat_atten = torch.mul(feat, atten)\n        feat_out = feat_atten + feat\n        return feat_out\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.modules.batchnorm._BatchNorm):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass TexturePath(nn.Module):\n    def __init__(self):\n        super(TexturePath, self).__init__()\n        self.conv1 = ConvBNReLU(3, 64, ks=7, stride=2, padding=3)\n        self.conv2 = ConvBNReLU(64, 64, ks=3, stride=2, padding=1)\n        self.conv3 = ConvBNReLU(64, 64, ks=3, stride=2, padding=1)\n        self.conv_out = ConvBNReLU(64, 128, ks=1, stride=1, padding=0)\n        self.init_weight()\n\n    def forward(self, x):\n        feat = self.conv1(x)\n        feat = self.conv2(feat)\n        feat = self.conv3(feat)\n        feat = self.conv_out(feat)\n        return feat\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.modules.batchnorm._BatchNorm):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass DependencyPath(nn.Module):\n    def __init__(self, weight_path='pretrain_weights/rest_lite.pth'):\n        super(DependencyPath, self).__init__()\n        self.ResT = rest_lite(weight_path=weight_path)\n        self.AE = Attention_Embedding(512, 256)\n        self.conv_avg = ConvBNReLU(256, 128, ks=1, stride=1, padding=0)\n        self.up = nn.Upsample(scale_factor=2.)\n\n    def forward(self, x):\n        e3, e4 = self.ResT(x)\n\n        e = self.conv_avg(self.AE(e4, e3))\n\n        return self.up(e)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.modules.batchnorm._BatchNorm):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass DependencyPathRes(nn.Module):\n    def __init__(self):\n        super(DependencyPathRes, self).__init__()\n        resnet = models.resnet18(True)\n        self.firstconv = resnet.conv1\n        self.firstbn = resnet.bn1\n        self.firstrelu = resnet.relu\n        self.firstmaxpool = resnet.maxpool\n        self.encoder1 = resnet.layer1\n        self.encoder2 = resnet.layer2\n        self.encoder3 = resnet.layer3\n        self.encoder4 = resnet.layer4\n        self.AE = Attention_Embedding(512, 256)\n        self.conv_avg = ConvBNReLU(256, 128, ks=1, stride=1, padding=0)\n        self.up = nn.Upsample(scale_factor=2.)\n\n    def forward(self, x):\n        x1 = self.firstconv(x)\n        x1 = self.firstbn(x1)\n        x1 = self.firstrelu(x1)\n        x1 = self.firstmaxpool(x1)\n        e1 = self.encoder1(x1)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n\n        e = self.conv_avg(self.AE(e4, e3))\n\n        return self.up(e)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.modules.batchnorm._BatchNorm):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass BANet(nn.Module):\n    def __init__(self, num_classes=6, weight_path= None):\n        super(BANet, self).__init__()    # path of pretrained weight file of ResT-lite  or None, recommend use.\n        self.name = 'BANet'\n\n        self.cp = DependencyPath(weight_path=weight_path)\n        self.sp = TexturePath()\n        self.fam = FeatureAggregationModule(256, 256)\n        self.conv_out = Output(256, 256, num_classes, up_factor=8)\n        self.init_weight()\n\n    def forward(self, x):\n        feat = self.cp(x)\n        feat_sp = self.sp(x)\n        feat_fuse = self.fam(feat_sp, feat)\n\n        feat_out = self.conv_out(feat_fuse)\n\n        return feat_out\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = [], [], [], []\n        for name, child in self.named_children():\n            child_wd_params, child_nowd_params = child.get_params()\n            if isinstance(child, (Attention_Embedding, Output)):\n                lr_mul_wd_params += child_wd_params\n                lr_mul_nowd_params += child_nowd_params\n            else:\n                wd_params += child_wd_params\n                nowd_params += child_nowd_params\n        return wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params","metadata":{"papermill":{"duration":0.026209,"end_time":"2023-11-13T07:12:28.552346","exception":false,"start_time":"2023-11-13T07:12:28.526137","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss function","metadata":{"papermill":{"duration":0.013249,"end_time":"2023-11-13T07:12:28.579095","exception":false,"start_time":"2023-11-13T07:12:28.565846","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def label_smoothed_nll_loss(\n    lprobs: torch.Tensor, target: torch.Tensor, epsilon: float, ignore_index=None, reduction=\"mean\", dim=-1\n) -> torch.Tensor:\n    \"\"\"\n\n    Source: https://github.com/pytorch/fairseq/blob/master/fairseq/criterions/label_smoothed_cross_entropy.py\n\n    :param lprobs: Log-probabilities of predictions (e.g after log_softmax)\n    :param target:\n    :param epsilon:\n    :param ignore_index:\n    :param reduction:\n    :return:\n    \"\"\"\n    if target.dim() == lprobs.dim() - 1:\n        target = target.unsqueeze(dim)\n\n    if ignore_index is not None:\n        pad_mask = target.eq(ignore_index)\n        target = target.masked_fill(pad_mask, 0)\n        nll_loss = -lprobs.gather(dim=dim, index=target)\n        smooth_loss = -lprobs.sum(dim=dim, keepdim=True)\n\n        # nll_loss.masked_fill_(pad_mask, 0.0)\n        # smooth_loss.masked_fill_(pad_mask, 0.0)\n        nll_loss = nll_loss.masked_fill(pad_mask, 0.0)\n        smooth_loss = smooth_loss.masked_fill(pad_mask, 0.0)\n    else:\n        nll_loss = -lprobs.gather(dim=dim, index=target)\n        smooth_loss = -lprobs.sum(dim=dim, keepdim=True)\n\n        nll_loss = nll_loss.squeeze(dim)\n        smooth_loss = smooth_loss.squeeze(dim)\n\n    if reduction == \"sum\":\n        nll_loss = nll_loss.sum()\n        smooth_loss = smooth_loss.sum()\n    if reduction == \"mean\":\n        nll_loss = nll_loss.mean()\n        smooth_loss = smooth_loss.mean()\n\n    eps_i = epsilon / lprobs.size(dim)\n    loss = (1.0 - epsilon) * nll_loss + eps_i * smooth_loss\n    return loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"__all__ = [\"SoftCrossEntropyLoss\"]\nfrom typing import Optional\nfrom torch.nn.modules.loss import _Loss\nfrom typing import List\nclass SoftCrossEntropyLoss(nn.Module):\n    \"\"\"\n    Drop-in replacement for nn.CrossEntropyLoss with few additions:\n    - Support of label smoothing\n    \"\"\"\n\n    __constants__ = [\"reduction\", \"ignore_index\", \"smooth_factor\"]\n\n    def __init__(self, reduction: str = \"mean\", smooth_factor: float = 0.0, ignore_index: Optional[int] = -100, dim=1):\n        super().__init__()\n        self.smooth_factor = smooth_factor\n        self.ignore_index = ignore_index\n        self.reduction = reduction\n        self.dim = dim\n\n    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n        log_prob = F.log_softmax(input, dim=self.dim)\n        pad_mask = target.eq(self.ignore_index)\n        target = target.masked_fill(pad_mask, 0)\n        log_prob = log_prob.masked_fill(pad_mask.unsqueeze(1), 0)\n        return label_smoothed_nll_loss(\n            log_prob,\n            target,\n            epsilon=self.smooth_factor,\n            ignore_index=self.ignore_index,\n            reduction=self.reduction,\n            dim=self.dim,\n        )\n__all__ = [\"JointLoss\", \"WeightedLoss\"]\n\n\nclass WeightedLoss(_Loss):\n    \"\"\"Wrapper class around loss function that applies weighted with fixed factor.\n    This class helps to balance multiple losses if they have different scales\n    \"\"\"\n\n    def __init__(self, loss, weight=1.0):\n        super().__init__()\n        self.loss = loss\n        self.weight = weight\n\n    def forward(self, *input):\n        return self.loss(*input) * self.weight\nclass JointLoss(_Loss):\n    \"\"\"\n    Wrap two loss functions into one. This class computes a weighted sum of two losses.\n    \"\"\"\n\n    def __init__(self, first: nn.Module, second: nn.Module, first_weight=1.0, second_weight=1.0):\n        super().__init__()\n        self.first = WeightedLoss(first, first_weight)\n        self.second = WeightedLoss(second, second_weight)\n\n    def forward(self, *input):\n        return self.first(*input) + self.second(*input)\n__all__ = [\"DiceLoss\"]\n\nBINARY_MODE = \"binary\"\nMULTICLASS_MODE = \"multiclass\"\nMULTILABEL_MODE = \"multilabel\"\ndef soft_dice_score(\n    output: torch.Tensor, target: torch.Tensor, smooth: float = 0.0, eps: float = 1e-7, dims=None\n) -> torch.Tensor:\n    \"\"\"\n\n    :param output:\n    :param target:\n    :param smooth:\n    :param eps:\n    :return:\n\n    Shape:\n        - Input: :math:`(N, NC, *)` where :math:`*` means any number\n            of additional dimensions\n        - Target: :math:`(N, NC, *)`, same shape as the input\n        - Output: scalar.\n\n    \"\"\"\n    assert output.size() == target.size()\n    if dims is not None:\n        intersection = torch.sum(output * target, dim=dims)\n        cardinality = torch.sum(output + target, dim=dims)\n    else:\n        intersection = torch.sum(output * target)\n        cardinality = torch.sum(output + target)\n    dice_score = (2.0 * intersection + smooth) / (cardinality + smooth).clamp_min(eps)\n    return dice_score\nclass DiceLoss(_Loss):\n    \"\"\"\n    Implementation of Dice loss for image segmentation task.\n    It supports binary, multiclass and multilabel cases\n    \"\"\"\n\n    def __init__(\n        self,\n        mode: str = 'multiclass',\n        classes: List[int] = None,\n        log_loss=False,\n        from_logits=True,\n        smooth: float = 0.0,\n        ignore_index=None,\n        eps=1e-7,\n    ):\n        \"\"\"\n\n        :param mode: Metric mode {'binary', 'multiclass', 'multilabel'}\n        :param classes: Optional list of classes that contribute in loss computation;\n        By default, all channels are included.\n        :param log_loss: If True, loss computed as `-log(jaccard)`; otherwise `1 - jaccard`\n        :param from_logits: If True assumes input is raw logits\n        :param smooth:\n        :param ignore_index: Label that indicates ignored pixels (does not contribute to loss)\n        :param eps: Small epsilon for numerical stability\n        \"\"\"\n        assert mode in {BINARY_MODE, MULTILABEL_MODE, MULTICLASS_MODE}\n        super(DiceLoss, self).__init__()\n        self.mode = mode\n        if classes is not None:\n            assert mode != BINARY_MODE, \"Masking classes is not supported with mode=binary\"\n            classes = to_tensor(classes, dtype=torch.long)\n\n        self.classes = classes\n        self.from_logits = from_logits\n        self.smooth = smooth\n        self.eps = eps\n        self.ignore_index = ignore_index\n        self.log_loss = log_loss\n\n    def forward(self, y_pred: Tensor, y_true: Tensor) -> Tensor:\n        \"\"\"\n\n        :param y_pred: NxCxHxW\n        :param y_true: NxHxW\n        :return: scalar\n        \"\"\"\n        assert y_true.size(0) == y_pred.size(0)\n\n        if self.from_logits:\n            # Apply activations to get [0..1] class probabilities\n            # Using Log-Exp as this gives more numerically stable result and does not cause vanishing gradient on\n            # extreme values 0 and 1\n            if self.mode == MULTICLASS_MODE:\n                y_pred = y_pred.log_softmax(dim=1).exp()\n            else:\n                y_pred = F.logsigmoid(y_pred).exp()\n\n        bs = y_true.size(0)\n        num_classes = y_pred.size(1)\n        dims = (0, 2)\n\n        if self.mode == BINARY_MODE:\n            y_true = y_true.view(bs, 1, -1)\n            y_pred = y_pred.view(bs, 1, -1)\n\n            if self.ignore_index is not None:\n                mask = y_true != self.ignore_index\n                y_pred = y_pred * mask\n                y_true = y_true * mask\n\n        if self.mode == MULTICLASS_MODE:\n            y_true = y_true.view(bs, -1)\n            y_pred = y_pred.view(bs, num_classes, -1)\n\n            if self.ignore_index is not None:\n                mask = y_true != self.ignore_index\n                y_pred = y_pred * mask.unsqueeze(1)\n\n                y_true = F.one_hot((y_true * mask).to(torch.long), num_classes)  # N,H*W -> N,H*W, C\n                y_true = y_true.permute(0, 2, 1) * mask.unsqueeze(1)  # H, C, H*W\n            else:\n                y_true = F.one_hot(y_true, num_classes)  # N,H*W -> N,H*W, C\n                y_true = y_true.permute(0, 2, 1)  # H, C, H*W\n\n        if self.mode == MULTILABEL_MODE:\n            y_true = y_true.view(bs, num_classes, -1)\n            y_pred = y_pred.view(bs, num_classes, -1)\n\n            if self.ignore_index is not None:\n                mask = y_true != self.ignore_index\n                y_pred = y_pred * mask\n                y_true = y_true * mask\n\n        scores = soft_dice_score(y_pred, y_true.type_as(y_pred), smooth=self.smooth, eps=self.eps, dims=dims)\n\n        if self.log_loss:\n            loss = -torch.log(scores.clamp_min(self.eps))\n        else:\n            loss = 1.0 - scores\n\n        # Dice loss is undefined for non-empty classes\n        # So we zero contribution of channel that does not have true pixels\n        # NOTE: A better workaround would be to use loss term `mean(y_pred)`\n        # for this case, however it will be a modified jaccard loss\n\n        mask = y_true.sum(dims) > 0\n        loss *= mask.to(loss.dtype)\n\n        if self.classes is not None:\n            loss = loss[self.classes]\n\n        return loss.mean()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass UnetLoss(nn.Module):\n    def __init__(self, ignore_index=255):\n        super().__init__()\n        self.main_loss = JointLoss(SoftCrossEntropyLoss(smooth_factor=0.05, ignore_index=ignore_index),\n                                   DiceLoss(smooth=0.05, ignore_index=ignore_index), 1.0, 1.0)\n        self.aux_loss = SoftCrossEntropyLoss(smooth_factor=0.05, ignore_index=ignore_index)\n\n    def forward(self, logits, labels):\n        if self.training and len(logits) == 2:\n            logit_main, logit_aux = logits\n            loss = self.main_loss(logit_main, labels) + 0.4 * self.aux_loss(logit_aux, labels)\n        else:\n            loss = self.main_loss(logits, labels)\n\n        return loss","metadata":{"papermill":{"duration":0.027653,"end_time":"2023-11-13T07:12:28.620311","exception":false,"start_time":"2023-11-13T07:12:28.592658","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"papermill":{"duration":0.013344,"end_time":"2023-11-13T07:12:28.647307","exception":false,"start_time":"2023-11-13T07:12:28.633963","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**Initialize weights**","metadata":{"papermill":{"duration":0.014389,"end_time":"2023-11-13T07:12:28.675129","exception":false,"start_time":"2023-11-13T07:12:28.66074","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def weights_init(model):\n    if isinstance(model, nn.Linear):\n        # Xavier Distribution\n        torch.nn.init.xavier_uniform_(model.weight)","metadata":{"papermill":{"duration":0.020816,"end_time":"2023-11-13T07:12:28.709398","exception":false,"start_time":"2023-11-13T07:12:28.688582","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_model(model, optimizer, path):\n    checkpoint = {\n        \"model\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    torch.save(checkpoint, path)\n\ndef load_model(model, optimizer, path):\n    checkpoint = torch.load(path)\n    model.load_state_dict(checkpoint[\"model\"])\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    return model, optimizer","metadata":{"papermill":{"duration":0.021664,"end_time":"2023-11-13T07:12:28.74459","exception":false,"start_time":"2023-11-13T07:12:28.722926","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train model**","metadata":{"papermill":{"duration":0.01333,"end_time":"2023-11-13T07:12:28.771998","exception":false,"start_time":"2023-11-13T07:12:28.758668","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def train(train_dataloader, valid_dataloader, learing_rate_scheduler, epoch, display_step):\n    print(f\"Start epoch #{epoch+1}, learning rate for this epoch: {learing_rate_scheduler.get_last_lr()}\")\n    start_time = time.time()\n    train_loss_epoch = 0\n    test_loss_epoch = 0\n    last_loss = 999999999\n    model.train()\n    metrics_train = Evaluator(num_class=6)\n    metrics_val = Evaluator(num_class=6)\n\n    for i,input in enumerate(tqdm(train_dataloader)):\n        # Load data into GPU\n        data=input['img']\n        masks_true = input['gt_semantic_seg']\n        data,mask = data.to(device),masks_true.to(device)\n        optimizer.zero_grad()\n        prediction = model(data)\n        # Backpropagation, compute gradients\n        loss = loss_function(prediction, mask.long())\n        pre_mask = nn.Softmax(dim=1)(prediction)\n        pre_mask = pre_mask.argmax(dim=1)\n        for i in range(mask.shape[0]):\n            metrics_train.add_batch(mask[i].cpu().numpy(), pre_mask[i].cpu().numpy())\n        loss.backward()\n\n        # Apply gradients\n        optimizer.step()\n\n        # Save loss\n        train_loss_epoch += loss.item()\n    print(f\"Done epoch #{epoch+1}, time for this epoch: {time.time()-start_time}s\")\n    train_loss_epoch /= (i + 1)\n    mIoU = np.nanmean(metrics_train.Intersection_over_Union()[:-1])\n    F1 = np.nanmean(metrics_train.F1()[:-1])\n    OA = np.nanmean(metrics_train.OA())\n    iou_per_class = metrics_train.Intersection_over_Union()\n    train_eval_value =  (iou_per_class,mIoU,F1,OA)\n    metrics_train.reset()\n    # Evaluate the validation set\n    model.eval()\n    with torch.no_grad():\n        for k,input in enumerate(tqdm(valid_dataloader)):\n            data=input['img'].to(device)\n            mask = input['gt_semantic_seg'].to(device)\n            prediction = model(data)\n            test_loss = loss_function(prediction, mask.long())\n            pre_mask = nn.Softmax(dim=1)(prediction)\n            pre_mask = pre_mask.argmax(dim=1)\n            test_loss_epoch += test_loss.item()\n            if k<=3:\n            # Convert predictions to 2D array\n                predictions_2d = pre_mask[0].cpu().numpy()  # Assuming you want the first prediction\n\n            # Convert ground truth masks to 2D array\n                masks_true_2d = mask[0].cpu().numpy()  # Assuming you want the first ground truth mask\n\n            # Convert to np.int8 if needed\n                predictions_2d = predictions_2d.astype(np.int8)\n                masks_true_2d = masks_true_2d.astype(np.int8)\n                wandb.log(\n      {f\"val_image{k}\" : wandb.Image(data[0], masks={\n        \"predictions\" : {\n            \"mask_data\" : predictions_2d,\n            \"class_labels\" : class_label\n        },\n        \"ground_truth\" : {\n            \"mask_data\" : masks_true_2d,\n            \"class_labels\" : class_label\n        }\n    })})\n            for i in range(mask.shape[0]):\n                metrics_val.add_batch(mask[i].cpu().numpy(), pre_mask[i].cpu().numpy())\n    test_loss_epoch /= (i + 1)\n    mIoU = np.nanmean(metrics_val.Intersection_over_Union()[:-1])\n    F1 = np.nanmean(metrics_val.F1()[:-1])\n    OA = np.nanmean(metrics_val.OA())\n    iou_per_class_val = metrics_val.Intersection_over_Union()\n    eval_value =  (iou_per_class_val,mIoU,F1,OA)\n    print(eval_value)\n    metrics_val.reset()\n    return train_loss_epoch, train_eval_value, test_loss_epoch, eval_value\n","metadata":{"papermill":{"duration":0.025915,"end_time":"2023-11-13T07:12:28.811424","exception":false,"start_time":"2023-11-13T07:12:28.785509","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Test model**","metadata":{}},{"cell_type":"code","source":"device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BANet(6)\nmodel.to(device)","metadata":{"papermill":{"duration":4.022563,"end_time":"2023-11-13T07:12:32.882726","exception":false,"start_time":"2023-11-13T07:12:28.860163","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# Check if CUDA is available\nif torch.cuda.is_available():\n    # Use GPU-accelerated PyTorch functions here\n    print(\"CUDA is available.\")\nelse:\n    print(\"CUDA is not available.\")","metadata":{"papermill":{"duration":0.014071,"end_time":"2023-11-13T07:12:32.911241","exception":false,"start_time":"2023-11-13T07:12:32.89717","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nloss_function = UnetLoss(ignore_index=6)\n# Define the optimizer (Adam optimizer)\noptimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n# optimizer.load_state_dict(checkpoint['optimizer'])\n\n# Learning rate scheduler\nlearing_rate_scheduler = lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.6)","metadata":{"papermill":{"duration":0.023208,"end_time":"2023-11-13T07:12:32.948265","exception":false,"start_time":"2023-11-13T07:12:32.925057","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_model(model, optimizer, checkpoint_path)\nload_checkpoint_flag=False","metadata":{"papermill":{"duration":0.92582,"end_time":"2023-11-13T07:12:33.887741","exception":false,"start_time":"2023-11-13T07:12:32.961921","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"Model keys:\")\n# print(model.state_dict().keys())\n\n# print(\"\\nCheckpoint keys:\")\n# print(checkpoint['model'].keys())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the model checkpoint if needed\n# Load the model checkpoint if needed\nif load_checkpoint_flag:\n    model, optimize= load_model(model, optimizer, pretrained_path)\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.login(\n    # set the wandb project where this run will be logged\n#     project= \"PolypSegment\", \n    key = \"dca7ab6f0e419f212da80de9d29ad8b7b38d1aa5\",\n)\nid = wandb.util.generate_id()\nwandb.init(id=id,project = \"test_hang \", resume=\"allow\")\n\n\n\n# Training loop\ntrain_loss_array = []\ntest_loss_array = []\nlast_loss = 9999999999999\nfor epoch in range(epochs):\n    train_loss_epoch = 0\n    test_loss_epoch = 0\n    train_loss_epoch, train_eval_value, test_loss_epoch, eval_value = train(train_loader, \n                                              val_loader, \n                                              learing_rate_scheduler, epoch, display_step)\n    \n    if test_loss_epoch < last_loss:\n        save_model(model, optimizer, checkpoint_path)\n        last_loss = test_loss_epoch\n        \n    iou_value = {}\n    iou_per_class,mIoU,F1,OA=train_eval_value\n    eval_value_train = {'mIoU': mIoU,\n                      'F1': F1,\n                      'OA': OA}                                          \n    print('train:', eval_value_train)\n    train_accuracy.append(OA)\n    wandb.log({'mIoU_train': mIoU,\n                      'F1_train': F1,\n                      'OA_train': OA}) \n    \n    for class_name, iou in zip(CLASSES,iou_per_class):\n        wandb.log({f\"{class_name}_train_IOU\": iou})\n        iou_value[class_name] = iou\n    print(iou_value)\n           \n    iou_value = {}\n    iou_per_class,mIoU,F1,OA = eval_value\n    eval_value_val = {'mIoU': mIoU,\n                      'F1': F1,\n                      'OA': OA}                                          \n    print('val:', eval_value_val)\n    valid_accuracy.append(OA)\n    wandb.log({'mIoU_val': mIoU,\n                      'F1_val': F1,\n                      'OA_val': OA})\n    for class_name, iou in zip(CLASSES,iou_per_class):\n        wandb.log({f\"{class_name}_val_IoU\": iou})\n        iou_value[class_name] = iou\n    print(iou_value)\n    \n    learing_rate_scheduler.step()\n    train_loss_array.append(train_loss_epoch)\n    test_loss_array.append(test_loss_epoch)\n    wandb.log({\"Train loss\": train_loss_epoch, \"Valid loss\": test_loss_epoch})\n    print(\"Epoch {}: loss: {:.4f}, train accuracy: {:.4f}, valid accuracy:{:.4f}\".format(epoch + 1, \n                                        train_loss_array[-1], train_accuracy[-1], valid_accuracy[-1]))\n    ","metadata":{"papermill":{"duration":7726.212208,"end_time":"2023-11-13T09:21:20.114357","exception":false,"start_time":"2023-11-13T07:12:33.902149","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"papermill":{"duration":0.03199,"end_time":"2023-11-13T09:21:20.171938","exception":false,"start_time":"2023-11-13T09:21:20.139948","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def img_writer(inp):\n    (mask,  mask_id, rgb) = inp\n    if rgb:\n        mask_name_tif = mask_id + '.png'\n        mask_tif = label2rgb(mask)\n        cv2.imwrite(mask_name_tif, mask_tif)\n    else:\n        mask_png = mask.astype(np.uint8)\n        mask_name_png = mask_id + '.png'\n        cv2.imwrite(mask_name_png, mask_png)\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    arg = parser.add_argument\n    arg(\"-c\", \"--config_path\", type=Path, required=True, help=\"Path to  config\")\n    arg(\"-o\", \"--output_path\", type=Path, help=\"Path where to save resulting masks.\", required=True)\n    arg(\"-t\", \"--tta\", help=\"Test time augmentation.\", default=None, choices=[None, \"d4\", \"lr\"])\n    arg(\"--rgb\", help=\"whether output rgb images\", action='store_true')\n    return parser.parse_args()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = PotsdamDataset(data_root='/kaggle/input/deep-data-potsdam-vaihingen/Deep_data_segmentation/Split/Vaihingen/Test',\n                              transform=val_aug)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.cuda()\nmodel.eval()\nevaluator = Evaluator(num_class=6)\nevaluator.reset()\nwith torch.no_grad():\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=2,\n        num_workers=4,\n        pin_memory=True,\n        drop_last=False,\n    )\n    results = []\n    for input in tqdm(test_loader):\n        # raw_prediction NxCxHxW\n        raw_predictions = model(input['img'].cuda())\n\n        image_ids = input[\"img_id\"]\n        masks_true = input['gt_semantic_seg']\n\n        raw_predictions = nn.Softmax(dim=1)(raw_predictions)\n        predictions = raw_predictions.argmax(dim=1)\n\n        for i in range(raw_predictions.shape[0]):\n            mask = predictions[i].cpu().numpy()\n            evaluator.add_batch(pre_image=mask, gt_image=masks_true[i].cpu().numpy())\n            mask_name = image_ids[i]\niou_per_class = evaluator.Intersection_over_Union()\nf1_per_class = evaluator.F1()\nOA = evaluator.OA()\nfor class_name, class_iou, class_f1 in zip(CLASSES, iou_per_class, f1_per_class):\n    print('F1_{}:{}, IOU_{}:{}'.format(class_name, class_f1, class_name, class_iou))\nprint('F1:{}, mIOU:{}, OA:{}'.format(np.nanmean(f1_per_class[:-1]), np.nanmean(iou_per_class[:-1]), OA))\nt0 = time.time()\nmpp.Pool(processes=2).map(img_writer, results)\nt1 = time.time()\nimg_write_time = t1 - t0\nprint('images writing spends: {} s'.format(img_write_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}