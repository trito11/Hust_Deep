{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7153471,"sourceType":"datasetVersion","datasetId":4130659}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":7774.718192,"end_time":"2023-11-13T09:21:28.081859","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-11-13T07:11:53.363667","version":"2.4.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n!pip install torchgeometry\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":5.358224,"end_time":"2023-11-13T07:12:26.531689","exception":false,"start_time":"2023-11-13T07:12:21.173465","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip show torch\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom torchgeometry.losses import one_hot\nimport os\nimport os.path as osp\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport cv2\nimport time\nimport imageio\nimport matplotlib.pyplot as plt\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch import Tensor\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision.transforms import Resize, PILToTensor, ToPILImage, Compose, InterpolationMode\nfrom collections import OrderedDict\nimport wandb\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport glob\nimport multiprocessing.pool as mpp\nimport multiprocessing as mp\nimport argparse\nimport torch\nimport albumentations as albu\nfrom torchvision.transforms import (Pad, ColorJitter, Resize, FiveCrop, RandomResizedCrop,\n                                    RandomHorizontalFlip, RandomRotation, RandomVerticalFlip)\nimport random\nimport math\nimport numbers\nfrom PIL import Image, ImageOps, ImageEnhance\nimport numpy as np\nimport random\nfrom scipy.ndimage.morphology import generate_binary_structure, binary_erosion\nfrom scipy.ndimage import maximum_filter\nfrom tqdm import tqdm\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport os\nimport torch\nfrom torch import nn\nimport cv2\nimport numpy as np\nimport argparse\nfrom pathlib import Path\n\nfrom pytorch_lightning.loggers import CSVLogger\nimport random","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"papermill":{"duration":0.082489,"end_time":"2023-11-13T07:12:27.60011","exception":false,"start_time":"2023-11-13T07:12:27.517621","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parameters","metadata":{"papermill":{"duration":0.013953,"end_time":"2023-11-13T07:12:27.627781","exception":false,"start_time":"2023-11-13T07:12:27.613828","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nnum_classes = 6\n\n# Number of epoch\nepochs = 10\n\n# Hyperparameters for training \nlearning_rate = 8e-04\nbatch_size = 8\ndisplay_step = 2\n\n# Model path\ncheckpoint_path = '/kaggle/working/unet_model.pth'\npretrained_path = \"/kaggle/input/unet-checkpoint/unet_model.pth\"\n# Initialize lists to keep track of loss and accuracy\nloss_epoch_array = []\ntrain_accuracy = []\ntest_accuracy = []\nvalid_accuracy = []","metadata":{"papermill":{"duration":0.021712,"end_time":"2023-11-13T07:12:27.662876","exception":false,"start_time":"2023-11-13T07:12:27.641164","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 42\n\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\n\nImSurf = np.array([255, 255, 255])  # label 0\nBuilding = np.array([255, 0, 0]) # label 1\nLowVeg = np.array([255, 255, 0]) # label 2\nTree = np.array([0, 255, 0]) # label 3\nCar = np.array([0, 255, 255]) # label 4\nClutter = np.array([0, 0, 255]) # label 5\nBoundary = np.array([0, 0, 0]) # label 6\nnum_classes = 6\n\n\n\n\n\ndef pv2rgb(mask):\n    h, w = mask.shape[0], mask.shape[1]\n    mask_rgb = np.zeros(shape=(h, w, 3), dtype=np.uint8)\n    mask_convert = mask[np.newaxis, :, :]\n    mask_rgb[np.all(mask_convert == 3, axis=0)] = [0, 255, 0]\n    mask_rgb[np.all(mask_convert == 0, axis=0)] = [255, 255, 255]\n    mask_rgb[np.all(mask_convert == 1, axis=0)] = [255, 0, 0]\n    mask_rgb[np.all(mask_convert == 2, axis=0)] = [255, 255, 0]\n    mask_rgb[np.all(mask_convert == 4, axis=0)] = [0, 204, 255]\n    mask_rgb[np.all(mask_convert == 5, axis=0)] = [0, 0, 255]\n    return mask_rgb\ndef label2rgb(mask):\n    h, w = mask.shape[0], mask.shape[1]\n    mask_rgb = np.zeros(shape=(h, w, 3), dtype=np.uint8)\n    mask_convert = mask[np.newaxis, :, :]\n    mask_rgb[np.all(mask_convert == 3, axis=0)] = [0, 255, 0]\n    mask_rgb[np.all(mask_convert == 0, axis=0)] = [255, 255, 255]\n    mask_rgb[np.all(mask_convert == 1, axis=0)] = [255, 0, 0]\n    mask_rgb[np.all(mask_convert == 2, axis=0)] = [255, 255, 0]\n    mask_rgb[np.all(mask_convert == 4, axis=0)] = [0, 204, 255]\n    mask_rgb[np.all(mask_convert == 5, axis=0)] = [0, 0, 255]\n    return mask_rgb\n\n\n\ndef car_color_replace(mask):\n    mask = cv2.cvtColor(np.array(mask.copy()), cv2.COLOR_RGB2BGR)\n    mask[np.all(mask == [0, 255, 255], axis=-1)] = [0, 204, 255]\n\n    return mask\n\n\ndef rgb_to_2D_label(_label):\n    _label = _label.transpose(2, 0, 1)\n    label_seg = np.zeros(_label.shape[1:], dtype=np.uint8)\n    label_seg[np.all(_label.transpose([1, 2, 0]) == ImSurf, axis=-1)] = 0\n    label_seg[np.all(_label.transpose([1, 2, 0]) == Building, axis=-1)] = 1\n    label_seg[np.all(_label.transpose([1, 2, 0]) == LowVeg, axis=-1)] = 2\n    label_seg[np.all(_label.transpose([1, 2, 0]) == Tree, axis=-1)] = 3\n    label_seg[np.all(_label.transpose([1, 2, 0]) == Car, axis=-1)] = 4\n    label_seg[np.all(_label.transpose([1, 2, 0]) == Clutter, axis=-1)] = 5\n    label_seg[np.all(_label.transpose([1, 2, 0]) == Boundary, axis=-1)] = 6\n    return label_seg\n\n\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # Split Data","metadata":{}},{"cell_type":"markdown","source":" # Dataloader","metadata":{"papermill":{"duration":0.013261,"end_time":"2023-11-13T07:12:27.690633","exception":false,"start_time":"2023-11-13T07:12:27.677372","status":"completed"},"tags":[]}},{"cell_type":"code","source":"CLASSES = ('ImSurf', 'Building', 'LowVeg', 'Tree', 'Car', 'Clutter')\nPALETTE = [[255, 255, 255], [0, 0, 255], [0, 255, 255], [0, 255, 0], [255, 204, 0], [255, 0, 0]]\n\nORIGIN_IMG_SIZE = (256, 256)\nINPUT_IMG_SIZE = (256, 256)\nTEST_IMG_SIZE = (256, 256)\n\ndef get_training_transform():\n    train_transform = [\n        # albu.RandomBrightnessContrast(brightness_limit=0.25, contrast_limit=0.25, p=0.15),\n        # albu.RandomRotate90(p=0.25),\n        albu.Normalize()\n    ]\n    return albu.Compose(train_transform)\n\ndef get_val_transform():\n    val_transform = [\n        albu.Normalize()\n    ]\n    return albu.Compose(val_transform)\n\n\ndef val_aug(img, mask):\n    img, mask = np.array(img), np.array(mask)\n    aug = get_val_transform()(image=img.copy(), mask=mask.copy())\n    img, mask = aug['image'], aug['mask']\n    return img, mask\n\n\nclass PotsdamDataset(Dataset):\n    def __init__(self, data_root='/kaggle/input/Deep_data_segmentation/Split/Potsdam/', mode='val', img_dir='Image', mask_dir='Label/',\n                 img_suffix='.tif', mask_suffix='.png', transform=val_aug, mosaic_ratio=0.0,\n                 img_size=ORIGIN_IMG_SIZE):\n        self.data_root = data_root\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.img_suffix = img_suffix\n        self.mask_suffix = mask_suffix\n        self.transform = transform\n        self.mode = mode\n        self.mosaic_ratio = mosaic_ratio\n        self.img_size = img_size\n        self.img_ids = self.get_img_ids(self.data_root, self.img_dir, self.mask_dir)\n\n    def __getitem__(self, index):\n        p_ratio = random.random()\n        if p_ratio > self.mosaic_ratio or self.mode == 'val' or self.mode == 'test':\n            img, mask = self.load_img_and_mask(index)\n            if self.transform:\n                img, mask = self.transform(img, mask)\n        else:\n            img, mask = self.load_mosaic_img_and_mask(index)\n            if self.transform:\n                img, mask = self.transform(img, mask)\n\n        img = torch.from_numpy(img).permute(2, 0, 1).float()\n        mask = torch.from_numpy(mask).long()\n        img_id = self.img_ids[index]\n        results = dict(img_id=img_id, img=img, gt_semantic_seg=mask)\n        return results\n\n    def __len__(self):\n        return len(self.img_ids)\n\n    def get_img_ids(self, data_root, img_dir, mask_dir):\n        img_filename_list = os.listdir(osp.join(data_root, img_dir))\n        mask_filename_list = os.listdir(osp.join(data_root, mask_dir))\n        assert len(img_filename_list) == len(mask_filename_list)\n        img_ids = [str(id.split('.')[0]) for id in mask_filename_list]\n        return img_ids\n\n    def load_img_and_mask(self, index):\n        img_id = self.img_ids[index]\n        img_name = osp.join(self.data_root, self.img_dir, img_id + self.img_suffix)\n        mask_name = osp.join(self.data_root, self.mask_dir, img_id + self.mask_suffix)\n        img = Image.open(img_name).convert('RGB')\n        mask = Image.open(mask_name).convert('L')\n        return img, mask\n\n    def load_mosaic_img_and_mask(self, index):\n        indexes = [index] + [random.randint(0, len(self.img_ids) - 1) for _ in range(3)]\n        img_a, mask_a = self.load_img_and_mask(indexes[0])\n        img_b, mask_b = self.load_img_and_mask(indexes[1])\n        img_c, mask_c = self.load_img_and_mask(indexes[2])\n        img_d, mask_d = self.load_img_and_mask(indexes[3])\n\n        img_a, mask_a = np.array(img_a), np.array(mask_a)\n        img_b, mask_b = np.array(img_b), np.array(mask_b)\n        img_c, mask_c = np.array(img_c), np.array(mask_c)\n        img_d, mask_d = np.array(img_d), np.array(mask_d)\n\n        w = self.img_size[1]\n        h = self.img_size[0]\n\n        start_x = w // 4\n        strat_y = h // 4\n        # The coordinates of the splice center\n        offset_x = random.randint(start_x, (w - start_x))\n        offset_y = random.randint(strat_y, (h - strat_y))\n\n        crop_size_a = (offset_x, offset_y)\n        crop_size_b = (w - offset_x, offset_y)\n        crop_size_c = (offset_x, h - offset_y)\n        crop_size_d = (w - offset_x, h - offset_y)\n\n        random_crop_a = albu.RandomCrop(width=crop_size_a[0], height=crop_size_a[1])\n        random_crop_b = albu.RandomCrop(width=crop_size_b[0], height=crop_size_b[1])\n        random_crop_c = albu.RandomCrop(width=crop_size_c[0], height=crop_size_c[1])\n        random_crop_d = albu.RandomCrop(width=crop_size_d[0], height=crop_size_d[1])\n\n        croped_a = random_crop_a(image=img_a.copy(), mask=mask_a.copy())\n        croped_b = random_crop_b(image=img_b.copy(), mask=mask_b.copy())\n        croped_c = random_crop_c(image=img_c.copy(), mask=mask_c.copy())\n        croped_d = random_crop_d(image=img_d.copy(), mask=mask_d.copy())\n\n        img_crop_a, mask_crop_a = croped_a['image'], croped_a['mask']\n        img_crop_b, mask_crop_b = croped_b['image'], croped_b['mask']\n        img_crop_c, mask_crop_c = croped_c['image'], croped_c['mask']\n        img_crop_d, mask_crop_d = croped_d['image'], croped_d['mask']\n\n        top = np.concatenate((img_crop_a, img_crop_b), axis=1)\n        bottom = np.concatenate((img_crop_c, img_crop_d), axis=1)\n        img = np.concatenate((top, bottom), axis=0)\n\n        top_mask = np.concatenate((mask_crop_a, mask_crop_b), axis=1)\n        bottom_mask = np.concatenate((mask_crop_c, mask_crop_d), axis=1)\n        mask = np.concatenate((top_mask, bottom_mask), axis=0)\n        mask = np.ascontiguousarray(mask)\n        img = np.ascontiguousarray(img)\n\n        img = Image.fromarray(img)\n        mask = Image.fromarray(mask)\n\n        return img, mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_root =\"/kaggle/input/Deep_data_segmentation/Split/Potsdam/Image/top_potsdam_4_12_0_25.tif\"\n\n\nif os.path.exists(data_root):\n    print(f\"The path '{data_root}' exists.\")\nelse:\n    print(f\"The path '{data_root}' does not exist.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_dataset = PotsdamDataset(data_root='/kaggle/input/Deep_data_segmentation/Split/Potsdam/', mode='train',\n                               mosaic_ratio=0.25, transform=val_aug)\n\nval_dataset = PotsdamDataset(transform=val_aug)\ntest_dataset = PotsdamDataset(data_root='/kaggle/input/Deep_data_segmentation/Split/Potsdam/Test',\n                              transform=val_aug)\n\ntrain_loader = DataLoader(dataset=train_dataset,\n                          batch_size=batch_size,\n                          num_workers=0,\n                          pin_memory=False,\n                          shuffle=True,\n                          drop_last=True)\n\nval_loader = DataLoader(dataset=val_dataset,\n                        batch_size=batch_size,\n                        num_workers=2,\n                        shuffle=False,\n                        pin_memory=True,\n                        drop_last=False)\n","metadata":{"papermill":{"duration":0.019707,"end_time":"2023-11-13T07:12:27.79702","exception":false,"start_time":"2023-11-13T07:12:27.777313","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metric","metadata":{}},{"cell_type":"code","source":"class Evaluator(object):\n    def __init__(self, num_class):\n        self.num_class = num_class\n        self.confusion_matrix = np.zeros((self.num_class,) * 2)\n        self.eps = 1e-8\n\n    def get_tp_fp_tn_fn(self):\n        tp = np.diag(self.confusion_matrix)\n        fp = self.confusion_matrix.sum(axis=0) - np.diag(self.confusion_matrix)\n        fn = self.confusion_matrix.sum(axis=1) - np.diag(self.confusion_matrix)\n        tn = np.diag(self.confusion_matrix).sum() - np.diag(self.confusion_matrix)\n        return tp, fp, tn, fn\n\n    def Precision(self):\n        tp, fp, tn, fn = self.get_tp_fp_tn_fn()\n        precision = tp / (tp + fp)\n        return precision\n\n    def Recall(self):\n        tp, fp, tn, fn = self.get_tp_fp_tn_fn()\n        recall = tp / (tp + fn)\n        return recall\n\n    def F1(self):\n        tp, fp, tn, fn = self.get_tp_fp_tn_fn()\n        Precision = tp / (tp + fp)\n        Recall = tp / (tp + fn)\n        F1 = (2.0 * Precision * Recall) / (Precision + Recall)\n        return F1\n\n    def OA(self):\n        OA = np.diag(self.confusion_matrix).sum() / (self.confusion_matrix.sum() + self.eps)\n        return OA\n\n    def Intersection_over_Union(self):\n        tp, fp, tn, fn = self.get_tp_fp_tn_fn()\n        IoU = tp / (tp + fn + fp)\n        return IoU\n\n    def Dice(self):\n        tp, fp, tn, fn = self.get_tp_fp_tn_fn()\n        Dice = 2 * tp / ((tp + fp) + (tp + fn))\n        return Dice\n\n    def Pixel_Accuracy_Class(self):\n        #         TP                                  TP+FP\n        Acc = np.diag(self.confusion_matrix) / (self.confusion_matrix.sum(axis=0) + self.eps)\n        return Acc\n\n    def Frequency_Weighted_Intersection_over_Union(self):\n        freq = np.sum(self.confusion_matrix, axis=1) / (np.sum(self.confusion_matrix) + self.eps)\n        iou = self.Intersection_over_Union()\n        FWIoU = (freq[freq > 0] * iou[freq > 0]).sum()\n        return FWIoU\n\n    def _generate_matrix(self, gt_image, pre_image):\n        mask = (gt_image >= 0) & (gt_image < self.num_class)\n        label = self.num_class * gt_image[mask].astype('int') + pre_image[mask]\n        count = np.bincount(label, minlength=self.num_class ** 2)\n        confusion_matrix = count.reshape(self.num_class, self.num_class)\n        return confusion_matrix\n\n    def add_batch(self, gt_image, pre_image):\n        assert gt_image.shape == pre_image.shape, 'pre_image shape {}, gt_image shape {}'.format(pre_image.shape,\n                                                                                                 gt_image.shape)\n        self.confusion_matrix += self._generate_matrix(gt_image, pre_image)\n\n    def reset(self):\n        self.confusion_matrix = np.zeros((self.num_class,) * 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install einops","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"papermill":{"duration":0.013405,"end_time":"2023-11-13T07:12:28.293069","exception":false,"start_time":"2023-11-13T07:12:28.279664","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**Encoder Block**","metadata":{"papermill":{"duration":0.013187,"end_time":"2023-11-13T07:12:28.319611","exception":false,"start_time":"2023-11-13T07:12:28.306424","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\n\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nimport timm\n\n\nclass ConvBNReLU(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, stride=1, norm_layer=nn.BatchNorm2d, bias=False):\n        super(ConvBNReLU, self).__init__(\n            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=bias,\n                      dilation=dilation, stride=stride, padding=((stride - 1) + dilation * (kernel_size - 1)) // 2),\n            norm_layer(out_channels),\n            nn.ReLU6()\n        )\n\n\nclass ConvBN(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, stride=1, norm_layer=nn.BatchNorm2d, bias=False):\n        super(ConvBN, self).__init__(\n            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=bias,\n                      dilation=dilation, stride=stride, padding=((stride - 1) + dilation * (kernel_size - 1)) // 2),\n            norm_layer(out_channels)\n        )\n\n\nclass Conv(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, stride=1, bias=False):\n        super(Conv, self).__init__(\n            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=bias,\n                      dilation=dilation, stride=stride, padding=((stride - 1) + dilation * (kernel_size - 1)) // 2)\n        )\n\n\nclass SeparableConvBNReLU(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1,\n                 norm_layer=nn.BatchNorm2d):\n        super(SeparableConvBNReLU, self).__init__(\n            nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, dilation=dilation,\n                      padding=((stride - 1) + dilation * (kernel_size - 1)) // 2,\n                      groups=in_channels, bias=False),\n            norm_layer(out_channels),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n            nn.ReLU6()\n        )\n\n\nclass SeparableConvBN(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1,\n                 norm_layer=nn.BatchNorm2d):\n        super(SeparableConvBN, self).__init__(\n            nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, dilation=dilation,\n                      padding=((stride - 1) + dilation * (kernel_size - 1)) // 2,\n                      groups=in_channels, bias=False),\n            norm_layer(out_channels),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        )\n\n\nclass SeparableConv(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1):\n        super(SeparableConv, self).__init__(\n            nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, dilation=dilation,\n                      padding=((stride - 1) + dilation * (kernel_size - 1)) // 2,\n                      groups=in_channels, bias=False),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        )\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.ReLU6, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Conv2d(in_features, hidden_features, 1, 1, 0, bias=True)\n        self.act = act_layer()\n        self.fc2 = nn.Conv2d(hidden_features, out_features, 1, 1, 0, bias=True)\n        self.drop = nn.Dropout(drop, inplace=True)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass GlobalLocalAttention(nn.Module):\n    def __init__(self,\n                 dim=256,\n                 num_heads=16,\n                 qkv_bias=False,\n                 window_size=8,\n                 relative_pos_embedding=True\n                 ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // self.num_heads\n        self.scale = head_dim ** -0.5\n        self.ws = window_size\n\n        self.qkv = Conv(dim, 3*dim, kernel_size=1, bias=qkv_bias)\n        self.local1 = ConvBN(dim, dim, kernel_size=3)\n        self.local2 = ConvBN(dim, dim, kernel_size=1)\n        self.proj = SeparableConvBN(dim, dim, kernel_size=window_size)\n\n        self.attn_x = nn.AvgPool2d(kernel_size=(window_size, 1), stride=1,  padding=(window_size//2 - 1, 0))\n        self.attn_y = nn.AvgPool2d(kernel_size=(1, window_size), stride=1, padding=(0, window_size//2 - 1))\n\n        self.relative_pos_embedding = relative_pos_embedding\n\n        if self.relative_pos_embedding:\n            # define a parameter table of relative position bias\n            self.relative_position_bias_table = nn.Parameter(\n                torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(self.ws)\n            coords_w = torch.arange(self.ws)\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += self.ws - 1  # shift to start from 0\n            relative_coords[:, :, 1] += self.ws - 1\n            relative_coords[:, :, 0] *= 2 * self.ws - 1\n            relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n\n            trunc_normal_(self.relative_position_bias_table, std=.02)\n\n    def pad(self, x, ps):\n        _, _, H, W = x.size()\n        if W % ps != 0:\n            x = F.pad(x, (0, ps - W % ps), mode='reflect')\n        if H % ps != 0:\n            x = F.pad(x, (0, 0, 0, ps - H % ps), mode='reflect')\n        return x\n\n    def pad_out(self, x):\n        x = F.pad(x, pad=(0, 1, 0, 1), mode='reflect')\n        return x\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n\n        local = self.local2(x) + self.local1(x)\n\n        x = self.pad(x, self.ws)\n        B, C, Hp, Wp = x.shape\n        qkv = self.qkv(x)\n\n        q, k, v = rearrange(qkv, 'b (qkv h d) (hh ws1) (ww ws2) -> qkv (b hh ww) h (ws1 ws2) d', h=self.num_heads,\n                            d=C//self.num_heads, hh=Hp//self.ws, ww=Wp//self.ws, qkv=3, ws1=self.ws, ws2=self.ws)\n\n        dots = (q @ k.transpose(-2, -1)) * self.scale\n\n        if self.relative_pos_embedding:\n            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                self.ws * self.ws, self.ws * self.ws, -1)  # Wh*Ww,Wh*Ww,nH\n            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n            dots += relative_position_bias.unsqueeze(0)\n\n        attn = dots.softmax(dim=-1)\n        attn = attn @ v\n\n        attn = rearrange(attn, '(b hh ww) h (ws1 ws2) d -> b (h d) (hh ws1) (ww ws2)', h=self.num_heads,\n                         d=C//self.num_heads, hh=Hp//self.ws, ww=Wp//self.ws, ws1=self.ws, ws2=self.ws)\n\n        attn = attn[:, :, :H, :W]\n\n        out = self.attn_x(F.pad(attn, pad=(0, 0, 0, 1), mode='reflect')) + \\\n              self.attn_y(F.pad(attn, pad=(0, 1, 0, 0), mode='reflect'))\n\n        out = out + local\n        out = self.pad_out(out)\n        out = self.proj(out)\n        # print(out.size())\n        out = out[:, :, :H, :W]\n\n        return out\n\n\nclass Block(nn.Module):\n    def __init__(self, dim=256, num_heads=16,  mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.ReLU6, norm_layer=nn.BatchNorm2d, window_size=8):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = GlobalLocalAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, window_size=window_size)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, out_features=dim, act_layer=act_layer, drop=drop)\n        self.norm2 = norm_layer(dim)\n\n    def forward(self, x):\n\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n\n\nclass WF(nn.Module):\n    def __init__(self, in_channels=128, decode_channels=128, eps=1e-8):\n        super(WF, self).__init__()\n        self.pre_conv = Conv(in_channels, decode_channels, kernel_size=1)\n\n        self.weights = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)\n        self.eps = eps\n        self.post_conv = ConvBNReLU(decode_channels, decode_channels, kernel_size=3)\n\n    def forward(self, x, res):\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n        weights = nn.ReLU()(self.weights)\n        fuse_weights = weights / (torch.sum(weights, dim=0) + self.eps)\n        x = fuse_weights[0] * self.pre_conv(res) + fuse_weights[1] * x\n        x = self.post_conv(x)\n        return x\n\n\nclass FeatureRefinementHead(nn.Module):\n    def __init__(self, in_channels=64, decode_channels=64):\n        super().__init__()\n        self.pre_conv = Conv(in_channels, decode_channels, kernel_size=1)\n\n        self.weights = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)\n        self.eps = 1e-8\n        self.post_conv = ConvBNReLU(decode_channels, decode_channels, kernel_size=3)\n\n        self.pa = nn.Sequential(nn.Conv2d(decode_channels, decode_channels, kernel_size=3, padding=1, groups=decode_channels),\n                                nn.Sigmoid())\n        self.ca = nn.Sequential(nn.AdaptiveAvgPool2d(1),\n                                Conv(decode_channels, decode_channels//16, kernel_size=1),\n                                nn.ReLU6(),\n                                Conv(decode_channels//16, decode_channels, kernel_size=1),\n                                nn.Sigmoid())\n\n        self.shortcut = ConvBN(decode_channels, decode_channels, kernel_size=1)\n        self.proj = SeparableConvBN(decode_channels, decode_channels, kernel_size=3)\n        self.act = nn.ReLU6()\n\n    def forward(self, x, res):\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n        weights = nn.ReLU()(self.weights)\n        fuse_weights = weights / (torch.sum(weights, dim=0) + self.eps)\n        x = fuse_weights[0] * self.pre_conv(res) + fuse_weights[1] * x\n        x = self.post_conv(x)\n        shortcut = self.shortcut(x)\n        pa = self.pa(x) * x\n        ca = self.ca(x) * x\n        x = pa + ca\n        x = self.proj(x) + shortcut\n        x = self.act(x)\n\n        return x\n\n\nclass AuxHead(nn.Module):\n\n    def __init__(self, in_channels=64, num_classes=8):\n        super().__init__()\n        self.conv = ConvBNReLU(in_channels, in_channels)\n        self.drop = nn.Dropout(0.1)\n        self.conv_out = Conv(in_channels, num_classes, kernel_size=1)\n\n    def forward(self, x, h, w):\n        feat = self.conv(x)\n        feat = self.drop(feat)\n        feat = self.conv_out(feat)\n        feat = F.interpolate(feat, size=(h, w), mode='bilinear', align_corners=False)\n        return feat\n\n\nclass Decoder(nn.Module):\n    def __init__(self,\n                 encoder_channels=(64, 128, 256, 512),\n                 decode_channels=64,\n                 dropout=0.1,\n                 window_size=8,\n                 num_classes=6):\n        super(Decoder, self).__init__()\n\n        self.pre_conv = ConvBN(encoder_channels[-1], decode_channels, kernel_size=1)\n        self.b4 = Block(dim=decode_channels, num_heads=8, window_size=window_size)\n\n        self.b3 = Block(dim=decode_channels, num_heads=8, window_size=window_size)\n        self.p3 = WF(encoder_channels[-2], decode_channels)\n\n        self.b2 = Block(dim=decode_channels, num_heads=8, window_size=window_size)\n        self.p2 = WF(encoder_channels[-3], decode_channels)\n\n        if self.training:\n            self.up4 = nn.UpsamplingBilinear2d(scale_factor=4)\n            self.up3 = nn.UpsamplingBilinear2d(scale_factor=2)\n            self.aux_head = AuxHead(decode_channels, num_classes)\n\n        self.p1 = FeatureRefinementHead(encoder_channels[-4], decode_channels)\n\n        self.segmentation_head = nn.Sequential(ConvBNReLU(decode_channels, decode_channels),\n                                               nn.Dropout2d(p=dropout, inplace=True),\n                                               Conv(decode_channels, num_classes, kernel_size=1))\n        self.init_weight()\n\n    def forward(self, res1, res2, res3, res4, h, w):\n        if self.training:\n            x = self.b4(self.pre_conv(res4))\n            h4 = self.up4(x)\n\n            x = self.p3(x, res3)\n            x = self.b3(x)\n            h3 = self.up3(x)\n\n            x = self.p2(x, res2)\n            x = self.b2(x)\n            h2 = x\n            x = self.p1(x, res1)\n            x = self.segmentation_head(x)\n            x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=False)\n\n            ah = h4 + h3 + h2\n            ah = self.aux_head(ah, h, w)\n\n            return x, ah\n        else:\n            x = self.b4(self.pre_conv(res4))\n            x = self.p3(x, res3)\n            x = self.b3(x)\n\n            x = self.p2(x, res2)\n            x = self.b2(x)\n\n            x = self.p1(x, res1)\n\n            x = self.segmentation_head(x)\n            x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=False)\n\n            return x\n\n    def init_weight(self):\n        for m in self.children():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, a=1)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n\nclass UNetFormer(nn.Module):\n    def __init__(self,\n                 decode_channels=64,\n                 dropout=0.1,\n                 backbone_name='swsl_resnet18',\n                 pretrained=True,\n                 window_size=8,\n                 num_classes=6\n                 ):\n        super().__init__()\n\n        self.backbone = timm.create_model(backbone_name, features_only=True, output_stride=32,\n                                          out_indices=(1, 2, 3, 4), pretrained=pretrained)\n        encoder_channels = self.backbone.feature_info.channels()\n\n        self.decoder = Decoder(encoder_channels, decode_channels, dropout, window_size, num_classes)\n\n    def forward(self, x):\n        h, w = x.size()[-2:]\n        res1, res2, res3, res4 = self.backbone(x)\n        if self.training:\n            x, ah = self.decoder(res1, res2, res3, res4, h, w)\n            return x, ah\n        else:\n            x = self.decoder(res1, res2, res3, res4, h, w)\n            return x","metadata":{"papermill":{"duration":0.024183,"end_time":"2023-11-13T07:12:28.357222","exception":false,"start_time":"2023-11-13T07:12:28.333039","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Decoder block**","metadata":{"papermill":{"duration":0.013371,"end_time":"2023-11-13T07:12:28.384103","exception":false,"start_time":"2023-11-13T07:12:28.370732","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.024252,"end_time":"2023-11-13T07:12:28.4219","exception":false,"start_time":"2023-11-13T07:12:28.397648","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Bottle neck**","metadata":{"papermill":{"duration":0.013422,"end_time":"2023-11-13T07:12:28.448754","exception":false,"start_time":"2023-11-13T07:12:28.435332","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.023287,"end_time":"2023-11-13T07:12:28.485664","exception":false,"start_time":"2023-11-13T07:12:28.462377","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Unet model**","metadata":{"papermill":{"duration":0.013331,"end_time":"2023-11-13T07:12:28.512584","exception":false,"start_time":"2023-11-13T07:12:28.499253","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.026209,"end_time":"2023-11-13T07:12:28.552346","exception":false,"start_time":"2023-11-13T07:12:28.526137","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# Instantiate your UNet model\nmodel = UNetFormer()\n\n\n# Print the shape of the outputmodel = UNet(n_class=6)\n\n# Create a dummy batch with random images\ndummy_batch_size = 16  # Choose your batch size\ndummy_input_batch = torch.randn(dummy_batch_size, 3, 256, 256)  # Batch size, 3 channels, 256x256\n\n# Forward pass\noutput_batch, _ = model(dummy_input_batch)\n\n# Print the shape of the output batch\n\nprint(\"Output batch shape:\", output_batch.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss function","metadata":{"papermill":{"duration":0.013249,"end_time":"2023-11-13T07:12:28.579095","exception":false,"start_time":"2023-11-13T07:12:28.565846","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def label_smoothed_nll_loss(\n    lprobs: torch.Tensor, target: torch.Tensor, epsilon: float, ignore_index=None, reduction=\"mean\", dim=-1\n) -> torch.Tensor:\n    \"\"\"\n\n    Source: https://github.com/pytorch/fairseq/blob/master/fairseq/criterions/label_smoothed_cross_entropy.py\n\n    :param lprobs: Log-probabilities of predictions (e.g after log_softmax)\n    :param target:\n    :param epsilon:\n    :param ignore_index:\n    :param reduction:\n    :return:\n    \"\"\"\n    if target.dim() == lprobs.dim() - 1:\n        target = target.unsqueeze(dim)\n\n    if ignore_index is not None:\n        pad_mask = target.eq(ignore_index)\n        target = target.masked_fill(pad_mask, 0)\n        nll_loss = -lprobs.gather(dim=dim, index=target)\n        smooth_loss = -lprobs.sum(dim=dim, keepdim=True)\n\n        # nll_loss.masked_fill_(pad_mask, 0.0)\n        # smooth_loss.masked_fill_(pad_mask, 0.0)\n        nll_loss = nll_loss.masked_fill(pad_mask, 0.0)\n        smooth_loss = smooth_loss.masked_fill(pad_mask, 0.0)\n    else:\n        nll_loss = -lprobs.gather(dim=dim, index=target)\n        smooth_loss = -lprobs.sum(dim=dim, keepdim=True)\n\n        nll_loss = nll_loss.squeeze(dim)\n        smooth_loss = smooth_loss.squeeze(dim)\n\n    if reduction == \"sum\":\n        nll_loss = nll_loss.sum()\n        smooth_loss = smooth_loss.sum()\n    if reduction == \"mean\":\n        nll_loss = nll_loss.mean()\n        smooth_loss = smooth_loss.mean()\n\n    eps_i = epsilon / lprobs.size(dim)\n    loss = (1.0 - epsilon) * nll_loss + eps_i * smooth_loss\n    return loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"__all__ = [\"SoftCrossEntropyLoss\"]\nfrom typing import Optional\nfrom torch.nn.modules.loss import _Loss\nfrom typing import List\nclass SoftCrossEntropyLoss(nn.Module):\n    \"\"\"\n    Drop-in replacement for nn.CrossEntropyLoss with few additions:\n    - Support of label smoothing\n    \"\"\"\n\n    __constants__ = [\"reduction\", \"ignore_index\", \"smooth_factor\"]\n\n    def __init__(self, reduction: str = \"mean\", smooth_factor: float = 0.0, ignore_index: Optional[int] = -100, dim=1):\n        super().__init__()\n        self.smooth_factor = smooth_factor\n        self.ignore_index = ignore_index\n        self.reduction = reduction\n        self.dim = dim\n\n    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n        log_prob = F.log_softmax(input, dim=self.dim)\n        pad_mask = target.eq(self.ignore_index)\n        target = target.masked_fill(pad_mask, 0)\n        log_prob = log_prob.masked_fill(pad_mask.unsqueeze(1), 0)\n        return label_smoothed_nll_loss(\n            log_prob,\n            target,\n            epsilon=self.smooth_factor,\n            ignore_index=self.ignore_index,\n            reduction=self.reduction,\n            dim=self.dim,\n        )\n__all__ = [\"JointLoss\", \"WeightedLoss\"]\n\n\nclass WeightedLoss(_Loss):\n    \"\"\"Wrapper class around loss function that applies weighted with fixed factor.\n    This class helps to balance multiple losses if they have different scales\n    \"\"\"\n\n    def __init__(self, loss, weight=1.0):\n        super().__init__()\n        self.loss = loss\n        self.weight = weight\n\n    def forward(self, *input):\n        return self.loss(*input) * self.weight\nclass JointLoss(_Loss):\n    \"\"\"\n    Wrap two loss functions into one. This class computes a weighted sum of two losses.\n    \"\"\"\n\n    def __init__(self, first: nn.Module, second: nn.Module, first_weight=1.0, second_weight=1.0):\n        super().__init__()\n        self.first = WeightedLoss(first, first_weight)\n        self.second = WeightedLoss(second, second_weight)\n\n    def forward(self, *input):\n        return self.first(*input) + self.second(*input)\n__all__ = [\"DiceLoss\"]\n\nBINARY_MODE = \"binary\"\nMULTICLASS_MODE = \"multiclass\"\nMULTILABEL_MODE = \"multilabel\"\ndef soft_dice_score(\n    output: torch.Tensor, target: torch.Tensor, smooth: float = 0.0, eps: float = 1e-7, dims=None\n) -> torch.Tensor:\n    \"\"\"\n\n    :param output:\n    :param target:\n    :param smooth:\n    :param eps:\n    :return:\n\n    Shape:\n        - Input: :math:`(N, NC, *)` where :math:`*` means any number\n            of additional dimensions\n        - Target: :math:`(N, NC, *)`, same shape as the input\n        - Output: scalar.\n\n    \"\"\"\n    assert output.size() == target.size()\n    if dims is not None:\n        intersection = torch.sum(output * target, dim=dims)\n        cardinality = torch.sum(output + target, dim=dims)\n    else:\n        intersection = torch.sum(output * target)\n        cardinality = torch.sum(output + target)\n    dice_score = (2.0 * intersection + smooth) / (cardinality + smooth).clamp_min(eps)\n    return dice_score\nclass DiceLoss(_Loss):\n    \"\"\"\n    Implementation of Dice loss for image segmentation task.\n    It supports binary, multiclass and multilabel cases\n    \"\"\"\n\n    def __init__(\n        self,\n        mode: str = 'multiclass',\n        classes: List[int] = None,\n        log_loss=False,\n        from_logits=True,\n        smooth: float = 0.0,\n        ignore_index=None,\n        eps=1e-7,\n    ):\n        \"\"\"\n\n        :param mode: Metric mode {'binary', 'multiclass', 'multilabel'}\n        :param classes: Optional list of classes that contribute in loss computation;\n        By default, all channels are included.\n        :param log_loss: If True, loss computed as `-log(jaccard)`; otherwise `1 - jaccard`\n        :param from_logits: If True assumes input is raw logits\n        :param smooth:\n        :param ignore_index: Label that indicates ignored pixels (does not contribute to loss)\n        :param eps: Small epsilon for numerical stability\n        \"\"\"\n        assert mode in {BINARY_MODE, MULTILABEL_MODE, MULTICLASS_MODE}\n        super(DiceLoss, self).__init__()\n        self.mode = mode\n        if classes is not None:\n            assert mode != BINARY_MODE, \"Masking classes is not supported with mode=binary\"\n            classes = to_tensor(classes, dtype=torch.long)\n\n        self.classes = classes\n        self.from_logits = from_logits\n        self.smooth = smooth\n        self.eps = eps\n        self.ignore_index = ignore_index\n        self.log_loss = log_loss\n\n    def forward(self, y_pred: Tensor, y_true: Tensor) -> Tensor:\n        \"\"\"\n\n        :param y_pred: NxCxHxW\n        :param y_true: NxHxW\n        :return: scalar\n        \"\"\"\n        assert y_true.size(0) == y_pred.size(0)\n\n        if self.from_logits:\n            # Apply activations to get [0..1] class probabilities\n            # Using Log-Exp as this gives more numerically stable result and does not cause vanishing gradient on\n            # extreme values 0 and 1\n            if self.mode == MULTICLASS_MODE:\n                y_pred = y_pred.log_softmax(dim=1).exp()\n            else:\n                y_pred = F.logsigmoid(y_pred).exp()\n\n        bs = y_true.size(0)\n        num_classes = y_pred.size(1)\n        dims = (0, 2)\n\n        if self.mode == BINARY_MODE:\n            y_true = y_true.view(bs, 1, -1)\n            y_pred = y_pred.view(bs, 1, -1)\n\n            if self.ignore_index is not None:\n                mask = y_true != self.ignore_index\n                y_pred = y_pred * mask\n                y_true = y_true * mask\n\n        if self.mode == MULTICLASS_MODE:\n            y_true = y_true.view(bs, -1)\n            y_pred = y_pred.view(bs, num_classes, -1)\n\n            if self.ignore_index is not None:\n                mask = y_true != self.ignore_index\n                y_pred = y_pred * mask.unsqueeze(1)\n\n                y_true = F.one_hot((y_true * mask).to(torch.long), num_classes)  # N,H*W -> N,H*W, C\n                y_true = y_true.permute(0, 2, 1) * mask.unsqueeze(1)  # H, C, H*W\n            else:\n                y_true = F.one_hot(y_true, num_classes)  # N,H*W -> N,H*W, C\n                y_true = y_true.permute(0, 2, 1)  # H, C, H*W\n\n        if self.mode == MULTILABEL_MODE:\n            y_true = y_true.view(bs, num_classes, -1)\n            y_pred = y_pred.view(bs, num_classes, -1)\n\n            if self.ignore_index is not None:\n                mask = y_true != self.ignore_index\n                y_pred = y_pred * mask\n                y_true = y_true * mask\n\n        scores = soft_dice_score(y_pred, y_true.type_as(y_pred), smooth=self.smooth, eps=self.eps, dims=dims)\n\n        if self.log_loss:\n            loss = -torch.log(scores.clamp_min(self.eps))\n        else:\n            loss = 1.0 - scores\n\n        # Dice loss is undefined for non-empty classes\n        # So we zero contribution of channel that does not have true pixels\n        # NOTE: A better workaround would be to use loss term `mean(y_pred)`\n        # for this case, however it will be a modified jaccard loss\n\n        mask = y_true.sum(dims) > 0\n        loss *= mask.to(loss.dtype)\n\n        if self.classes is not None:\n            loss = loss[self.classes]\n\n        return loss.mean()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass UnetLoss(nn.Module):\n    def __init__(self, ignore_index=255):\n        super().__init__()\n        self.main_loss = JointLoss(SoftCrossEntropyLoss(smooth_factor=0.05, ignore_index=ignore_index),\n                                   DiceLoss(smooth=0.05, ignore_index=ignore_index), 1.0, 1.0)\n        self.aux_loss = SoftCrossEntropyLoss(smooth_factor=0.05, ignore_index=ignore_index)\n\n    def forward(self, logits, labels):\n        if self.training and len(logits) == 2:\n            logit_main, logit_aux = logits\n            loss = self.main_loss(logit_main, labels) + 0.4 * self.aux_loss(logit_aux, labels)\n        else:\n            loss = self.main_loss(logits, labels)\n\n        return loss","metadata":{"papermill":{"duration":0.027653,"end_time":"2023-11-13T07:12:28.620311","exception":false,"start_time":"2023-11-13T07:12:28.592658","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"papermill":{"duration":0.013344,"end_time":"2023-11-13T07:12:28.647307","exception":false,"start_time":"2023-11-13T07:12:28.633963","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**Initialize weights**","metadata":{"papermill":{"duration":0.014389,"end_time":"2023-11-13T07:12:28.675129","exception":false,"start_time":"2023-11-13T07:12:28.66074","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def weights_init(model):\n    if isinstance(model, nn.Linear):\n        # Xavier Distribution\n        torch.nn.init.xavier_uniform_(model.weight)","metadata":{"papermill":{"duration":0.020816,"end_time":"2023-11-13T07:12:28.709398","exception":false,"start_time":"2023-11-13T07:12:28.688582","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_model(model, optimizer, path):\n    checkpoint = {\n        \"model\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    torch.save(checkpoint, path)\n\ndef load_model(model, optimizer, path):\n    checkpoint = torch.load(path)\n    model.load_state_dict(checkpoint[\"model\"])\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    return model, optimizer","metadata":{"papermill":{"duration":0.021664,"end_time":"2023-11-13T07:12:28.74459","exception":false,"start_time":"2023-11-13T07:12:28.722926","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train model**","metadata":{"papermill":{"duration":0.01333,"end_time":"2023-11-13T07:12:28.771998","exception":false,"start_time":"2023-11-13T07:12:28.758668","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def train(train_dataloader, valid_dataloader, learing_rate_scheduler, epoch, display_step):\n    print(f\"Start epoch #{epoch+1}, learning rate for this epoch: {learing_rate_scheduler.get_last_lr()}\")\n    start_time = time.time()\n    train_loss_epoch = 0\n    test_loss_epoch = 0\n    last_loss = 999999999\n    model.train()\n    metrics_train = Evaluator(num_class=6)\n    metrics_val = Evaluator(num_class=6)\n\n    for i,input in enumerate(tqdm(train_dataloader)):\n        # Load data into GPU\n        data=input['img']\n        masks_true = input['gt_semantic_seg']\n        data,mask = data.to(device),masks_true.to(device)\n        optimizer.zero_grad()\n        prediction, head = model(data)\n        # Backpropagation, compute gradients\n        loss = loss_function(prediction, mask.long())\n        pre_mask = nn.Softmax(dim=1)(prediction)\n        pre_mask = pre_mask.argmax(dim=1)\n        for i in range(mask.shape[0]):\n            metrics_train.add_batch(mask[i].cpu().numpy(), pre_mask[i].cpu().numpy())\n        loss.backward()\n\n        # Apply gradients\n        optimizer.step()\n\n        # Save loss\n        train_loss_epoch += loss.item()\n    print(f\"Done epoch #{epoch+1}, time for this epoch: {time.time()-start_time}s\")\n    train_loss_epoch /= (i + 1)\n    mIoU = np.nanmean(metrics_train.Intersection_over_Union()[:-1])\n    F1 = np.nanmean(metrics_train.F1()[:-1])\n    OA = np.nanmean(metrics_train.OA())\n    iou_per_class = metrics_train.Intersection_over_Union()\n    train_eval_value =  (iou_per_class,mIoU,F1,OA)\n    metrics_train.reset()\n    # Evaluate the validation set\n    model.eval()\n    with torch.no_grad():\n        for input in tqdm(valid_dataloader):\n            data=input['img'].to(device)\n            mask = input['gt_semantic_seg'].to(device)\n            prediction = model(data)\n            test_loss = loss_function(prediction, mask.long())\n            pre_mask = nn.Softmax(dim=1)(prediction)\n            pre_mask = pre_mask.argmax(dim=1)\n            test_loss_epoch += test_loss.item()\n            for i in range(mask.shape[0]):\n                metrics_val.add_batch(mask[i].cpu().numpy(), pre_mask[i].cpu().numpy())\n    test_loss_epoch /= (i + 1)\n    mIoU = np.nanmean(metrics_val.Intersection_over_Union()[:-1])\n    F1 = np.nanmean(metrics_val.F1()[:-1])\n    OA = np.nanmean(metrics_val.OA())\n    iou_per_class_val = metrics_val.Intersection_over_Union()\n    eval_value =  (iou_per_class_val,mIoU,F1,OA)\n    print(eval_value)\n    metrics_val.reset()\n    return train_loss_epoch, train_eval_value, test_loss_epoch, eval_value\n","metadata":{"papermill":{"duration":0.025915,"end_time":"2023-11-13T07:12:28.811424","exception":false,"start_time":"2023-11-13T07:12:28.785509","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Test model**","metadata":{}},{"cell_type":"code","source":"\ndef test(dataloader):\n    evaluator = Evaluator(num_class=6)\n    evaluator.reset()\n    results = []\n    for input in tqdm(dataloader):\n        # raw_prediction NxCxHxW\n        raw_predictions = model(input['img'].cuda())\n\n        image_ids = input[\"img_id\"]\n        masks_true = input['gt_semantic_seg']\n\n        raw_predictions = nn.Softmax(dim=1)(raw_predictions)\n        predictions = raw_predictions.argmax(dim=1)\n\n        for i in range(raw_predictions.shape[0]):\n            mask = predictions[i].cpu().numpy()\n            evaluator.add_batch(pre_image=mask, gt_image=masks_true[i].cpu().numpy())\n            mask_name = image_ids[i]\n            results.append((mask, str(args.output_path / mask_name), args.rgb))\n        for i in range(raw_predictions.shape[0]):\n            mask = predictions[i].cpu().numpy()\n            evaluator.add_batch(pre_image=mask, gt_image=masks_true[i].cpu().numpy())\n            mask_name = image_ids[i]\n            results.append((mask, str(args.output_path / mask_name), args.rgb))\n    iou_per_class = evaluator.Intersection_over_Union()\n    f1_per_class = evaluator.F1()\n    OA = evaluator.OA()\n    return iou_per_class, f1_per_class, OA","metadata":{"papermill":{"duration":0.021642,"end_time":"2023-11-13T07:12:28.84675","exception":false,"start_time":"2023-11-13T07:12:28.825108","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = UNetFormer()\nmodel.to(device)","metadata":{"papermill":{"duration":4.022563,"end_time":"2023-11-13T07:12:32.882726","exception":false,"start_time":"2023-11-13T07:12:28.860163","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# Check if CUDA is available\nif torch.cuda.is_available():\n    # Use GPU-accelerated PyTorch functions here\n    print(\"CUDA is available.\")\nelse:\n    print(\"CUDA is not available.\")","metadata":{"papermill":{"duration":0.014071,"end_time":"2023-11-13T07:12:32.911241","exception":false,"start_time":"2023-11-13T07:12:32.89717","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nloss_function = UnetLoss(ignore_index=6)\n# Define the optimizer (Adam optimizer)\noptimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n# optimizer.load_state_dict(checkpoint['optimizer'])\n\n# Learning rate scheduler\nlearing_rate_scheduler = lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.6)","metadata":{"papermill":{"duration":0.023208,"end_time":"2023-11-13T07:12:32.948265","exception":false,"start_time":"2023-11-13T07:12:32.925057","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_model(model, optimizer, checkpoint_path)\nload_checkpoint_flag=True","metadata":{"papermill":{"duration":0.92582,"end_time":"2023-11-13T07:12:33.887741","exception":false,"start_time":"2023-11-13T07:12:32.961921","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"Model keys:\")\n# print(model.state_dict().keys())\n\n# print(\"\\nCheckpoint keys:\")\n# print(checkpoint['model'].keys())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Load the model checkpoint if needed\n# # Load the model checkpoint if needed\n# if load_checkpoint_flag:\n#     checkpoint = torch.load(checkpoint_path)\n#     model, optimize= load_model(model, optimizer, checkpoint)\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.login(\n    # set the wandb project where this run will be logged\n#     project= \"PolypSegment\", \n    key = \"b98d2b806f364f5af900550ec98e26e2f418e8a7\",\n)\nwandb.init(\n    project = \"UnetFormer\"\n)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Training loop\ntrain_loss_array = []\ntest_loss_array = []\nlast_loss = 9999999999999\nfor epoch in range(30):\n    try:\n        train_loss_epoch = 0\n        test_loss_epoch = 0\n        train_loss_epoch, train_eval_value, test_loss_epoch, eval_value = train(train_loader, \n                                                  val_loader, \n                                                  learing_rate_scheduler, epoch, display_step)\n\n        if test_loss_epoch < last_loss:\n            save_model(model, optimizer, checkpoint_path)\n            last_loss = test_loss_epoch\n\n        iou_value = {}\n        iou_per_class,mIoU,F1,OA=train_eval_value\n        eval_value_train = {'mIoU': mIoU,\n                          'F1': F1,\n                          'OA': OA}                                          \n        print('train:', eval_value_train)\n        train_accuracy.append(OA)\n        wandb.log({'mIoU_train': mIoU,\n                          'F1_train': F1,\n                          'OA_train': OA}) \n\n        for class_name, iou in zip(CLASSES,iou_per_class):\n            wandb.log({f\"{class_name}_train_IOU\": iou})\n            iou_value[class_name] = iou\n        print(iou_value)\n\n        iou_value = {}\n        iou_per_class,mIoU,F1,OA = eval_value\n        eval_value_val = {'mIoU': mIoU,\n                          'F1': F1,\n                          'OA': OA}                                          \n        print('val:', eval_value_val)\n        valid_accuracy.append(OA)\n        wandb.log({'mIoU_val': mIoU,\n                          'F1_val': F1,\n                          'OA_val': OA})\n        for class_name, iou in zip(CLASSES,iou_per_class):\n            wandb.log({f\"{class_name}_val_IoU\": iou})\n            iou_value[class_name] = iou\n        print(iou_value)\n\n        learing_rate_scheduler.step()\n        train_loss_array.append(train_loss_epoch)\n        test_loss_array.append(test_loss_epoch)\n        wandb.log({\"Train loss\": train_loss_epoch, \"Valid loss\": test_loss_epoch})\n        print(\"Epoch {}: loss: {:.4f}, train accuracy: {:.4f}, valid accuracy:{:.4f}\".format(epoch + 1, \n                                            train_loss_array[-1], train_accuracy[-1], valid_accuracy[-1]))\n    except:\n        torch.cuda.empty_cache()\n    ","metadata":{"papermill":{"duration":7726.212208,"end_time":"2023-11-13T09:21:20.114357","exception":false,"start_time":"2023-11-13T07:12:33.902149","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"papermill":{"duration":0.03199,"end_time":"2023-11-13T09:21:20.171938","exception":false,"start_time":"2023-11-13T09:21:20.139948","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.024586,"end_time":"2023-11-13T09:21:20.221336","exception":false,"start_time":"2023-11-13T09:21:20.19675","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot the learning cure","metadata":{"papermill":{"duration":0.024534,"end_time":"2023-11-13T09:21:20.270588","exception":false,"start_time":"2023-11-13T09:21:20.246054","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# load_model(model, checkpoint)","metadata":{"papermill":{"duration":0.031504,"end_time":"2023-11-13T09:21:20.326709","exception":false,"start_time":"2023-11-13T09:21:20.295205","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.dpi'] = 90\nplt.rcParams['figure.figsize'] = (6, 4)\nepochs_array = range(epochs)","metadata":{"papermill":{"duration":0.031761,"end_time":"2023-11-13T09:21:20.383049","exception":false,"start_time":"2023-11-13T09:21:20.351288","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Training and Test loss\nplt.plot(epochs_array, train_loss_array, 'g', label='Training loss')\nplt.plot(epochs_array, test_loss_array, 'b', label='Test loss')\nplt.title('Training and Test loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"papermill":{"duration":0.324591,"end_time":"2023-11-13T09:21:20.732601","exception":false,"start_time":"2023-11-13T09:21:20.40801","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Infer**","metadata":{"papermill":{"duration":0.025171,"end_time":"2023-11-13T09:21:20.78381","exception":false,"start_time":"2023-11-13T09:21:20.758639","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# from torch.jit import load\n# model = UNet()\n# optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n\n# checkpoint = torch.load(pretrained_path)","metadata":{"papermill":{"duration":0.032171,"end_time":"2023-11-13T09:21:20.841338","exception":false,"start_time":"2023-11-13T09:21:20.809167","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer.load_state_dict(checkpoint['optimizer'])","metadata":{"papermill":{"duration":0.031681,"end_time":"2023-11-13T09:21:20.898418","exception":false,"start_time":"2023-11-13T09:21:20.866737","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from collections import OrderedDict\n# new_state_dict = OrderedDict()\n# for k, v in checkpoint['model'].items():\n#     name = k[7:] # remove `module.`\n#     new_state_dict[name] = v\n# # load params\n# model.load_state_dict(new_state_dict)","metadata":{"papermill":{"duration":0.032386,"end_time":"2023-11-13T09:21:20.956384","exception":false,"start_time":"2023-11-13T09:21:20.923998","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.025216,"end_time":"2023-11-13T09:21:21.007264","exception":false,"start_time":"2023-11-13T09:21:20.982048","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualize results**","metadata":{"papermill":{"duration":0.025228,"end_time":"2023-11-13T09:21:21.058135","exception":false,"start_time":"2023-11-13T09:21:21.032907","status":"completed"},"tags":[]}},{"cell_type":"code","source":"for i, (data, label) in enumerate(train_dataloader):\n    img = data\n    mask = label\n    break","metadata":{"papermill":{"duration":0.156265,"end_time":"2023-11-13T09:21:21.239924","exception":false,"start_time":"2023-11-13T09:21:21.083659","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, arr = plt.subplots(4, 3, figsize=(16, 12))\n# arr[0][0].set_title('Image')\n# arr[0][1].set_title('Segmentation')\n# arr[0][2].set_title('Predict')\n\n# model.eval()\n# with torch.no_grad():\n#     z = img.to(device)\n#     predict = model(z)\n\n# def display_image_grid(images_filenames, images_directory, masks_directory, predicted_masks=None):\n#     cols = 3 if predicted_masks else 2\n#     rows = len(images_filenames)\n#     figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(10, 24))\n#     for i, image_filename in enumerate(images_filenames):\n#         image = cv2.imread(os.path.join(images_directory, image_filename))\n#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n#         mask = cv2.imread(os.path.join(masks_directory, image_filename.replace(\".jpeg\", \".png\")), cv2.IMREAD_UNCHANGED,)\n#         mask = preprocess_mask(mask)\n#         ax[i, 0].imshow(image)\n#         ax[i, 1].imshow(mask, interpolation=\"nearest\")\n\n#         ax[i, 0].set_title(\"Image\")\n#         ax[i, 1].set_title(\"Ground truth mask\")\n\n#         ax[i, 0].set_axis_off()\n#         ax[i, 1].set_axis_off()\n\n#         if predicted_masks:\n#             predicted_mask = predicted_masks[i]\n#             ax[i, 2].imshow(predicted_mask, interpolation=\"nearest\")\n#             ax[i, 2].set_title(\"Predicted mask\")\n#             ax[i, 2].set_axis_off()\n#     plt.tight_layout()\n#     plt.show()\n\n# display_image_grid(, images_directory, masks_directory,predicted_masks = predict)    \n# # for i in range(2):\n# #     arr[i][0].imshow(img[i].permute(1, 2, 0));\n    \n# #     arr[i][1].imshow(F.one_hot(mask[i]).float())\n    \n# #     arr[i][2].imshow(F.one_hot(torch.argmax(predict[i], 0).cpu()).float())","metadata":{"papermill":{"duration":3.94406,"end_time":"2023-11-13T09:21:25.210255","exception":true,"start_time":"2023-11-13T09:21:21.266195","status":"failed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create submission**","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def img_writer(inp):\n    (mask,  mask_id, rgb) = inp\n    if rgb:\n        mask_name_tif = mask_id + '.png'\n        mask_tif = label2rgb(mask)\n        cv2.imwrite(mask_name_tif, mask_tif)\n    else:\n        mask_png = mask.astype(np.uint8)\n        mask_name_png = mask_id + '.png'\n        cv2.imwrite(mask_name_png, mask_png)\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    arg = parser.add_argument\n    arg(\"-c\", \"--config_path\", type=Path, required=True, help=\"Path to  config\")\n    arg(\"-o\", \"--output_path\", type=Path, help=\"Path where to save resulting masks.\", required=True)\n    arg(\"-t\", \"--tta\", help=\"Test time augmentation.\", default=None, choices=[None, \"d4\", \"lr\"])\n    arg(\"--rgb\", help=\"whether output rgb images\", action='store_true')\n    return parser.parse_args()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = get_args()\nseed_everything(42)\n\nconfig = py2cfg(args.config_path)\nargs.output_path.mkdir(exist_ok=True, parents=True)\n\nmodel = Supervision_Train.load_from_checkpoint(\n    os.path.join(config.weights_path, config.test_weights_name + '.ckpt'), config=config)\nmodel.cuda()\nmodel.eval()\nevaluator = Evaluator(num_class=config.num_classes)\nevaluator.reset()\nif args.tta == \"lr\":\n    transforms = tta.Compose(\n        [\n            tta.HorizontalFlip(),\n            tta.VerticalFlip()\n        ]\n    )\n    model = tta.SegmentationTTAWrapper(model, transforms)\nelif args.tta == \"d4\":\n    transforms = tta.Compose(\n        [\n            tta.HorizontalFlip(),\n            tta.VerticalFlip(),\n            # tta.Rotate90(angles=[90]),\n            tta.Scale(scales=[0.75, 1.0, 1.25, 1.5], interpolation='bicubic', align_corners=False)\n        ]\n    )\n    model = tta.SegmentationTTAWrapper(model, transforms)\n\ntest_dataset = config.test_dataset\n\nwith torch.no_grad():\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=2,\n        num_workers=4,\n        pin_memory=True,\n        drop_last=False,\n    )\n    results = []\n    for input in tqdm(test_loader):\n        # raw_prediction NxCxHxW\n        raw_predictions, _ = model(input['img'].cuda())\n\n        image_ids = input[\"img_id\"]\n        masks_true = input['gt_semantic_seg']\n\n        raw_predictions, _ = nn.Softmax(dim=1)(raw_predictions)\n        predictions = raw_predictions.argmax(dim=1)\n\n        for i in range(raw_predictions.shape[0]):\n            mask = predictions[i].cpu().numpy()\n            evaluator.add_batch(pre_image=mask, gt_image=masks_true[i].cpu().numpy())\n            mask_name = image_ids[i]\n            results.append((mask, str(args.output_path / mask_name), args.rgb))\niou_per_class = evaluator.Intersection_over_Union()\nf1_per_class = evaluator.F1()\nOA = evaluator.OA()\nfor class_name, class_iou, class_f1 in zip(CLASSES, iou_per_class, f1_per_class):\n    print('F1_{}:{}, IOU_{}:{}'.format(class_name, class_f1, class_name, class_iou))\nprint('F1:{}, mIOU:{}, OA:{}'.format(np.nanmean(f1_per_class[:-1]), np.nanmean(iou_per_class[:-1]), OA))\nt0 = time.time()\nmpp.Pool(processes=mp.cpu_count()).map(img_writer, results)\nt1 = time.time()\nimg_write_time = t1 - t0\nprint('images writing spends: {} s'.format(img_write_time))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = A.Compose([A.Resize(384, 384),\n                       A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n                     ToTensorV2(),])","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, arr = plt.subplots(5, 2, figsize=(16, 12))\n# arr[0][0].set_title('Image');\n# arr[0][1].set_title('Predict');\n\n# model.eval()\n# with torch.no_grad():\n#     z = img.to(device)\n#     predict = model(z)\n\n# for i in range(5):\n#     arr[i][0].imshow(img[i].permute(1, 2, 0));\n#     arr[i][1].imshow(F.one_hot(torch.argmax(predict[i], 0).cpu()).float())","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nif not os.path.isdir(\"/kaggle/working/predicted_masks\"):\n    os.mkdir(\"/kaggle/working/predicted_masks\")\nfor _, (img, path, H, W) in enumerate(test_dataloader):\n    a = path\n    b = img\n    h = H\n    w = W\n    \n    with torch.no_grad():\n        z = b.to(device)\n        predicted_mask = model(z)\n    for i in range(len(a)):\n        image_id = a[i].split('/')[-1].split('.')[0]\n        filename = image_id + \".png\"\n        mask2img = Resize((h[i].item(), w[i].item()), interpolation=InterpolationMode.NEAREST)(ToPILImage()(F.one_hot(torch.argmax(predicted_mask[i], 0)).permute(2, 0, 1).float()))\n        mask2img.save(os.path.join(\"/kaggle/working/predicted_masks/\", filename))","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rle_to_string(runs):\n    return ' '.join(str(x) for x in runs)\n\ndef rle_encode_one_mask(mask):\n    pixels = mask.flatten()\n    pixels[pixels > 0] = 255\n    use_padding = False\n    if pixels[0] or pixels[-1]:\n        use_padding = True\n        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n        pixel_padded[1:-1] = pixels\n        pixels = pixel_padded\n    \n    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    if use_padding:\n        rle = rle - 1\n    rle[1::2] = rle[1::2] - rle[:-1:2]\n    return rle_to_string(rle)\n\ndef mask2string(dir):\n    ## mask --> string\n    strings = []\n    ids = []\n    ws, hs = [[] for i in range(2)]\n    for image_id in os.listdir(dir):\n        id = image_id.split('.')[0]\n        path = os.path.join(dir, image_id)\n        print(path)\n        img = cv2.imread(path)[:,:,::-1]\n        h, w = img.shape[0], img.shape[1]\n        for channel in range(2):\n            ws.append(w)\n            hs.append(h)\n            ids.append(f'{id}_{channel}')\n            string = rle_encode_one_mask(img[:,:,channel])\n            strings.append(string)\n    r = {\n        'ids': ids,\n        'strings': strings,\n    }\n    return r\n\n\nMASK_DIR_PATH = '/kaggle/working/predicted_masks' # change this to the path to your output mask folder\ndir = MASK_DIR_PATH\nres = mask2string(dir)\ndf = pd.DataFrame(columns=['Id', 'Expected'])\ndf['Id'] = res['ids']\ndf['Expected'] = res['strings']\ndf.to_csv(r'output.csv', index=False)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}